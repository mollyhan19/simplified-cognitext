
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <script src="https://d3js.org/d3.v7.min.js"></script>
            <style>
                body {
                    font-family: Arial, sans-serif;
                    margin: 0;
                    overflow: hidden;
                }
                .explanation-panel {
                    position: absolute;
                    padding: 15px;
                    background: white;
                    border: 1px solid #ccc;
                    border-radius: 8px;
                    box-shadow: 0 2px 10px rgba(0,0,0,0.2);
                    max-width: 300px;
                    z-index: 1000;
                    font-size: 14px;
                    line-height: 1.4;
                    opacity: 0;
                    transition: opacity 0.3s;
                    pointer-events: auto;
                }
                
                .explanation-panel.visible {
                    opacity: 1;
                }
                
                .explanation-title {
                    margin-top: 0;
                    margin-bottom: 10px;
                    color: #2196F3;
                    font-size: 16px;
                    font-weight: bold;
                }
                
                .explanation-content {
                    margin-bottom: 10px;
                }
                
                .explanation-footer {
                    display: block;
                    margin-top: 8px;
                    font-style: italic;
                    color: #666;
                    font-size: 12px;
                }
                
                .close-explanation {
                    position: absolute;
                    top: 5px;
                    right: 5px;
                    background: none;
                    border: none;
                    font-size: 16px;
                    cursor: pointer;
                    color: #666;
                }
                .node {
                    cursor: pointer;
                }
                .node circle {
                    stroke-width: 2px;
                    transition: all 0.3s ease;
                }
                .node.has-explanation circle {
                    stroke-dasharray: 3, 3;
                }
                .node--pinned circle {
                    stroke-width: 3px;
                    stroke-dasharray: none;
                    stroke: #f06292;
                }
                .node--priority circle {
                    stroke: #7E57C2;
                }
                .node--secondary circle {
                    stroke: #6596B5;
                }
                .node--tertiary circle {
                    stroke: #8FC2B9;
                }
                .hidden-connections-highlight {
                    stroke: #FF8A65 !important;
                    stroke-width: 3px !important;
                }
                .node--expanded circle {
                    stroke-width: 3px;
                }
                .node text {
                    font: 12px sans-serif;
                    pointer-events: none;
                }
                .node.faded circle {
                    opacity: 0.2;
                    transition: opacity 0.2s ease;
                }
                
                .node.faded text {
                    opacity: 0.1;
                    transition: opacity 0.2s ease;
                }
                
                .link.faded {
                    opacity: 0.1;
                    transition: opacity 0.2s ease;
                }
                
                .node.focused circle {
                    stroke-width: 4px;
                    stroke: #FF5252;
                }
                .link {
                    fill: none;
                    stroke-width: 1.5px;
                    cursor: pointer;
                    transition: stroke 0.3s ease;
                }
                .link:hover {
                    stroke-width: 2.5px;
                    stroke-opacity: 0.9 !important;
                }
                .link-label {
                    font-size: 10px;
                    fill: #666;
                    pointer-events: none;
                }
                .tooltip {
                    position: absolute;
                    padding: 8px;
                    background: rgba(255, 255, 255, 0.95);
                    color: #333;
                    border-radius: 4px;
                    box-shadow: 0 1px 3px rgba(0,0,0,0.2);
                    pointer-events: none;
                    font-size: 12px;
                    max-width: 300px;
                    z-index: 1000;
                    border: 1px solid #ddd;
                }
                .evidence-tooltip {
                    max-width: 350px;
                    line-height: 1.4;
                }
                .tooltip .right-click-instruction {
                    display: block;
                    margin-top: 5px;
                    font-style: italic;
                    color: #2196F3;
                }
                .legend {
                    position: absolute;
                    top: 10px;
                    right: 10px;
                    background: rgba(255, 255, 255, 0.8);
                    border-radius: 4px;
                    padding: 8px;
                    box-shadow: 0 1px 3px rgba(0,0,0,0.2);
                }
                .legend-item {
                    display: flex;
                    align-items: center;
                    margin-bottom: 5px;
                }
                .legend-color {
                    width: 15px;
                    height: 15px;
                    border-radius: 50%;
                    margin-right: 8px;
                }
                .priority-color {
                    background-color: #9575CD;
                    border: 1.5px solid #7E57C2;
                }
                .secondary-color {
                    background-color: #97C0DB;
                    border: 1.5px solid #6596B5;
                }
                .tertiary-color {
                    background-color: #D1EDE8;
                    border: 1.5px solid #ABD9D1;
                }
                @keyframes pulse-1745436004660 {
                    0% { transform: scale(1); opacity: 0.5; }
                    50% { transform: scale(1.2); opacity: 0.2; }
                    100% { transform: scale(1); opacity: 0.5; }
                }
                .pulse-1745436004660 {
                    animation: pulse-1745436004660 2s infinite;
                }
                .pulse-ring.faded {
                    opacity: 0.01 !important;
                }
                .controls {
                    position: absolute;
                    top: 10px;
                    left: 10px;
                    display: flex;
                    gap: 10px;
                }
                button {
                    background-color: white;
                    border: 1px solid #ccc;
                    border-radius: 4px;
                    padding: 5px 10px;
                    cursor: pointer;
                    font-size: 12px;
                }
                button:hover {
                    background-color: #f0f0f0;
                }
                .center-node circle {
                    stroke-width: 3px;
                }
                .highlight {
                    font-weight: bold;
                    color: #006400;
                }
            </style>
        </head>
        <body>
            <div class="controls">
                <button id="reset-btn">Reset</button>
                <button id="expand-all-btn">Expand All</button>
                <button id="recenter-btn">Recenter</button>
                <button id="unpin-btn">Unpin All</button>
            </div>
            <div class="legend">
                <div class="legend-item">
                    <div class="legend-color priority-color"></div>
                    <span>Priority Concepts</span>
                </div>
                <div class="legend-item">
                    <div class="legend-color secondary-color"></div>
                    <span>Secondary Concepts</span>
                </div>
                <div class="legend-item">
                    <div class="legend-color tertiary-color"></div>
                    <span>Tertiary Concepts</span>
                </div>
                <div class="legend-item">
                    <div style="width:15px; height:15px; margin-right:8px; border:1.5px solid #FF8A65; border-radius:50%; background-color: rgba(255, 0, 0, 0.3);" class="pulse-1745436004660"></div>
                    <span>Has Hidden Connections</span>
                </div>
            </div>

            <svg id="concept-map" width="800" height="600"></svg>
            <div id="tooltip" class="tooltip" style="display: none;"></div>
            <div id="evidence-tooltip" class="tooltip evidence-tooltip" style="display: none;"></div>

            <script>
            // Load the data
            const networkData = {"nodes": [{"id": "polynomial time", "name": "polynomial time", "frequency": 8, "degree": 9, "layer": "priority", "evidence": "Polynomial time refers to the time complexity of an algorithm where the time taken to complete the task is a polynomial function of the size of the input. This concept is essential in classifying problems in computer science, particularly in the context of the P versus NP problem."}, {"id": "p versus np problem", "name": "p versus np problem", "frequency": 7, "degree": 28, "layer": "priority", "evidence": "The P versus NP problem is a fundamental question in theoretical computer science that explores the relationship between problems that can be quickly verified and those that can be quickly solved. Understanding this problem is crucial as it has implications across various fields, including mathematics and computer science."}, {"id": "p \u2260 np", "name": "p \u2260 np", "frequency": 7, "degree": 9, "layer": "priority", "evidence": "The assertion that P does not equal NP suggests that there exist problems for which finding a solution is inherently more complex than verifying one. This distinction is pivotal in understanding the limitations of algorithmic problem-solving."}, {"id": "np-complete problems", "name": "np-complete problems", "frequency": 6, "degree": 15, "layer": "priority", "evidence": "NP-complete problems are a subset of NP problems that are characterized by their reducibility to one another in polynomial time. This means that if you can solve one NP-complete problem efficiently, you can solve all NP problems efficiently. Understanding these problems is essential for researchers in computer science as they explore the boundaries of computational efficiency."}, {"id": "class p", "name": "class p", "frequency": 4, "degree": 8, "layer": "priority", "evidence": "Class P consists of decision problems that can be solved by an algorithm in polynomial time. This classification is significant as it helps in understanding which problems are efficiently solvable, forming a core part of computational theory."}, {"id": "class np", "name": "class np", "frequency": 4, "degree": 10, "layer": "priority", "evidence": "Class NP includes decision problems for which a proposed solution can be verified in polynomial time. This concept is critical in the study of computational complexity, particularly in relation to the P versus NP problem."}, {"id": "historical context of np-completeness", "name": "historical context of np-completeness", "frequency": 4, "degree": 6, "layer": "tertiary", "evidence": "The historical context of NP-completeness includes the development of the theory in the 1970s, which revolutionized the understanding of computational problems. The existence of NP-complete problems was initially counterintuitive, leading to significant research and exploration in the field. Understanding this context helps to appreciate the evolution of computational theory and its implications."}, {"id": "np-completeness", "name": "np-completeness", "frequency": 5, "degree": 7, "layer": "priority", "evidence": "NP-completeness is a fundamental concept in computational theory that categorizes certain problems based on their difficulty. An NP-complete problem is one that is at least as hard as the hardest problems in NP, meaning that if a polynomial-time solution exists for one NP-complete problem, it exists for all problems in NP. This concept is crucial for understanding the limits of efficient computation."}, {"id": "computational complexity theory", "name": "computational complexity theory", "frequency": 3, "degree": 6, "layer": "priority", "evidence": "Computational complexity theory is a branch of theoretical computer science that focuses on classifying computational problems based on their inherent difficulty and the resources needed to solve them. Understanding this theory is crucial for determining the efficiency of algorithms and the feasibility of solving various computational problems."}, {"id": "p = np", "name": "p = np", "frequency": 3, "degree": 9, "layer": "priority", "evidence": "P = NP is a major unsolved problem in computer science that asks whether every problem whose solution can be quickly verified can also be solved quickly. Its resolution could revolutionize fields such as cryptography, operations research, and mathematics, making previously intractable problems solvable."}, {"id": "millennium prize problems", "name": "millennium prize problems", "frequency": 2, "degree": 8, "layer": "priority", "evidence": "The Millennium Prize Problems are a set of seven unsolved problems in mathematics, each with a reward for a correct solution. The P versus NP problem is included in this list, highlighting its significance and the high level of interest it generates in the mathematical community."}, {"id": "algorithm research", "name": "algorithm research", "frequency": 2, "degree": 8, "layer": "secondary", "evidence": "Algorithm research focuses on developing methods for solving computational problems efficiently. The P versus NP problem is central to this field, as it questions the limits of what can be computed quickly."}, {"id": "finite number of possible grids", "name": "finite number of possible grids", "frequency": 2, "degree": 2, "layer": "secondary", "evidence": "The concept of a finite number of possible grids refers to the limited configurations available for a fixed-size Sudoku puzzle. This characteristic allows for simpler solutions, such as table lookup, which can be efficiently executed, contrasting with the complexities of the generalized version."}, {"id": "computational complexity", "name": "computational complexity", "frequency": 2, "degree": 9, "layer": "priority", "evidence": "Computational complexity is a field of study that focuses on classifying problems based on their inherent difficulty and the resources required to solve them. The generalized Sudoku problem serves as an important case study in this discipline, illustrating the challenges of algorithmic efficiency."}, {"id": "co-np-complete", "name": "co-np-complete", "frequency": 2, "degree": 3, "layer": "secondary", "evidence": "Co-NP-complete refers to a class of problems for which a solution can be verified in polynomial time, but finding a solution may be computationally difficult. Understanding co-NP-completeness is essential for grasping the broader implications of the P versus NP problem."}, {"id": "non-deterministic machine", "name": "non-deterministic machine", "frequency": 2, "degree": 3, "layer": "secondary", "evidence": "A non-deterministic machine is a theoretical model that can explore multiple possible solutions simultaneously. This concept is crucial in the context of NP problems, as it allows for the verification of solutions in polynomial time, even if finding those solutions may not be feasible in the same timeframe."}, {"id": "boolean satisfiability problem", "name": "boolean satisfiability problem", "frequency": 2, "degree": 4, "layer": "priority", "evidence": "The Boolean satisfiability problem (SAT) is the first problem that was proven to be NP-complete, as established by the Cook\u2013Levin theorem. This theorem is significant because it provides a foundational example of NP-completeness and serves as a basis for proving that many other problems are also NP-complete. SAT's importance lies in its role in theoretical computer science and practical applications in fields like artificial intelligence and optimization."}, {"id": "turing machine", "name": "turing machine", "frequency": 2, "degree": 0, "layer": "secondary", "evidence": "A Turing machine is a theoretical computational model that defines an abstract machine capable of simulating any algorithm. In the context of NP-completeness, Turing machines are used to formalize the concepts of computation and verification, making them fundamental to understanding the complexity of problems in NP."}, {"id": "3-sat", "name": "3-sat", "frequency": 2, "degree": 1, "layer": "secondary", "evidence": "3-SAT is a specific case of the Boolean satisfiability problem where each clause in the formula has exactly three literals. It is a well-known NP-complete problem and serves as a critical example in the study of computational complexity. The significance of 3-SAT lies in its role in reductions and its applications in various fields, including algorithm design and optimization."}, {"id": "big o notation", "name": "big o notation", "frequency": 2, "degree": 2, "layer": "priority", "evidence": "Big O notation is a mathematical notation used to describe the upper bound of an algorithm's running time or space requirements in terms of input size. It is crucial for analyzing algorithm efficiency, but it can sometimes obscure important details, such as constants that significantly impact practical performance."}, {"id": "np", "name": "np", "frequency": 2, "degree": 2, "layer": "priority", "evidence": "NP (nondeterministic polynomial time) is a complexity class that includes decision problems for which a proposed solution can be verified in polynomial time. Understanding NP is critical for exploring the boundaries of efficient computation and the implications of the P = NP question."}, {"id": "implications for various fields", "name": "implications for various fields", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "The resolution of the P versus NP problem would impact numerous disciplines, indicating its importance beyond theoretical computer science. Understanding its implications can help in grasping the broader significance of computational complexity."}, {"id": "theoretical computer science", "name": "theoretical computer science", "frequency": 1, "degree": 4, "layer": "secondary", "evidence": "Theoretical computer science is a branch of computer science that deals with the abstract and mathematical aspects of computation. The P versus NP problem is a cornerstone issue in this field, influencing various theoretical frameworks."}, {"id": "quickly verified", "name": "quickly verified", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "The term 'quickly verified' refers to the ability to check the correctness of a solution in a time-efficient manner. This concept is crucial in distinguishing between the classes P and NP."}, {"id": "clay mathematics institute", "name": "clay mathematics institute", "frequency": 1, "degree": 2, "layer": "tertiary", "evidence": "The Clay Mathematics Institute is an organization that promotes mathematical research and recognizes significant unsolved problems in mathematics. Their designation of the P versus NP problem underscores its importance in the field."}, {"id": "generalized sudoku problem", "name": "generalized sudoku problem", "frequency": 1, "degree": 5, "layer": "priority", "evidence": "The generalized Sudoku problem extends the traditional Sudoku puzzle to grids of size n^{2} by n^{2}. It poses the challenge of determining whether a valid solution exists for any given incomplete grid, which is significant in the study of computational complexity and algorithm design."}, {"id": "table lookup", "name": "table lookup", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "Table lookup is a method of solving problems by referencing precomputed solutions stored in a table. This approach is efficient for fixed-size Sudoku puzzles, illustrating a practical application of algorithmic strategies in computational problem-solving."}, {"id": "sudoku grid", "name": "sudoku grid", "frequency": 1, "degree": 2, "layer": "tertiary", "evidence": "A Sudoku grid is a square grid divided into smaller squares, where the objective is to fill the grid with numbers according to specific rules. Understanding the structure of the grid is essential for tackling Sudoku-related problems and algorithms."}, {"id": "stephen cook", "name": "stephen cook", "frequency": 1, "degree": 3, "layer": "tertiary", "evidence": "Stephen Cook is a prominent computer scientist known for his foundational work in computational complexity theory, particularly for formulating the P versus NP problem. His contributions have significantly shaped the understanding of computational limits and efficiency."}, {"id": "leonid levin", "name": "leonid levin", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "Leonid Levin is a mathematician and computer scientist who independently formulated the P versus NP problem around the same time as Stephen Cook. His work is crucial in the context of theoretical computer science and highlights the collaborative nature of scientific discovery."}, {"id": "john nash", "name": "john nash", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "John Nash was a mathematician whose insights into game theory and complexity have had a profound impact on economics and computer science. His speculation about the time complexity of code-breaking foreshadowed the P versus NP problem and its implications for cryptography."}, {"id": "kurt g\u00f6del", "name": "kurt g\u00f6del", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "Kurt G\u00f6del was a logician and mathematician known for his incompleteness theorems. His inquiry into the efficiency of theorem-proving processes relates to the P versus NP problem and emphasizes the quest for automating mathematical proofs."}, {"id": "exponential time", "name": "exponential time", "frequency": 1, "degree": 6, "layer": "secondary", "evidence": "Exponential time describes algorithms whose running time grows exponentially with the input size, making them impractical for large inputs. This concept is significant in understanding the limitations of computational problem-solving, particularly in cryptography."}, {"id": "automated discovery of mathematical proofs", "name": "automated discovery of mathematical proofs", "frequency": 1, "degree": 2, "layer": "secondary", "evidence": "The automated discovery of mathematical proofs refers to the use of algorithms and computational methods to generate proofs without human intervention. This concept is pivotal in the field of artificial intelligence and has implications for the future of mathematics and logic."}, {"id": "complexity classes p and np", "name": "complexity classes p and np", "frequency": 1, "degree": 4, "layer": "priority", "evidence": "Complexity classes P and NP categorize decision problems based on their solvability and verifiability. Class P includes problems that can be solved in polynomial time by a deterministic machine, while class NP includes problems for which solutions can be verified in polynomial time. This distinction is fundamental in understanding computational limits and the efficiency of algorithms."}, {"id": "deterministic machine", "name": "deterministic machine", "frequency": 1, "degree": 2, "layer": "secondary", "evidence": "A deterministic machine is a theoretical model of computation where the outcome of each operation is predictable and determined solely by the current state and input. This concept is essential for analyzing the time complexity of algorithms, as it provides a clear framework for understanding how problems are solved step by step."}, {"id": "william gasarch", "name": "william gasarch", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "William Gasarch is a researcher known for his work on the P vs NP problem and for conducting surveys to gauge the opinions of experts in the field. His contributions help to illuminate the current state of belief regarding this fundamental question in theoretical computer science."}, {"id": "poll results on p vs np", "name": "poll results on p vs np", "frequency": 1, "degree": 3, "layer": "tertiary", "evidence": "The poll results indicate a growing consensus among researchers that P is not equal to NP, reflecting the ongoing debate and uncertainty surrounding this critical question in computational complexity. These statistics provide insight into the evolving perspectives within the academic community."}, {"id": "subjective opinion of this era", "name": "subjective opinion of this era", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "The phrase 'subjective opinion of this era' refers to the varying beliefs and attitudes of researchers regarding the P vs NP question. It highlights the complexity and uncertainty surrounding this topic, emphasizing that while opinions may shift, the fundamental problem remains unresolved."}, {"id": "np-hard problems", "name": "np-hard problems", "frequency": 1, "degree": 3, "layer": "priority", "evidence": "NP-hard problems are a broader class of problems that are at least as difficult as NP problems, but they do not necessarily belong to NP themselves. This means that while all NP problems can be transformed into NP-hard problems, NP-hard problems may not have solutions that can be verified in polynomial time. This distinction is important for understanding the hierarchy of computational problems."}, {"id": "cook\u2013levin theorem", "name": "cook\u2013levin theorem", "frequency": 1, "degree": 5, "layer": "priority", "evidence": "The Cook\u2013Levin theorem is a landmark result in computational theory that established the Boolean satisfiability problem as NP-complete. This theorem is crucial because it laid the groundwork for the study of NP-completeness and provided a method for proving other problems' NP-completeness through reductions. Its implications are far-reaching in both theoretical and practical aspects of computer science."}, {"id": "proof by reduction", "name": "proof by reduction", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "Proof by reduction is a method used to demonstrate that one problem is at least as hard as another by transforming instances of one problem into instances of another. This technique is essential in the field of computational complexity as it allows researchers to classify problems based on their difficulty and to establish relationships between different problems, particularly in proving NP-completeness."}, {"id": "sudoku", "name": "sudoku", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "Sudoku is a popular puzzle that has been shown to be NP-complete through proof by reduction. This means that solving Sudoku efficiently would imply that all NP problems can be solved efficiently. The connection between Sudoku and NP-completeness illustrates how seemingly simple games can have deep implications in computational theory."}, {"id": "exptime", "name": "exptime", "frequency": 1, "degree": 0, "layer": "priority", "evidence": "EXPTIME refers to the class of decision problems that can be solved by a deterministic Turing machine in exponential time. This concept is important as it helps categorize problems based on their computational complexity, distinguishing them from those solvable in polynomial time."}, {"id": "exptime-complete", "name": "exptime-complete", "frequency": 1, "degree": 0, "layer": "priority", "evidence": "EXPTIME-complete problems are the hardest problems in the EXPTIME class, meaning that if any EXPTIME-complete problem can be solved in polynomial time, then all problems in EXPTIME can be solved in polynomial time. This classification is significant for understanding the limits of efficient computation."}, {"id": "time hierarchy theorem", "name": "time hierarchy theorem", "frequency": 1, "degree": 2, "layer": "priority", "evidence": "The time hierarchy theorem establishes that there are problems that require more time than others, specifically showing that there are problems solvable in exponential time that cannot be solved in polynomial time. This theorem is fundamental in theoretical computer science as it delineates the boundaries of computational complexity."}, {"id": "presburger arithmetic", "name": "presburger arithmetic", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "Presburger arithmetic is a decidable subset of arithmetic that deals with natural numbers and addition. The complexity of deciding statements in this arithmetic is significant because it highlights the limitations of algorithmic decision-making in certain mathematical frameworks."}, {"id": "undecidable problems", "name": "undecidable problems", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "Undecidable problems are those for which no algorithm can provide a solution for all possible inputs. The halting problem is a classic example, demonstrating fundamental limits of computation and the inherent challenges in algorithm design."}, {"id": "#p problems", "name": "#p problems", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "#P problems are a class of counting problems that ask how many solutions exist for a given decision problem. They are significant because they extend the complexity theory beyond decision problems, revealing deeper insights into computational difficulty."}, {"id": "#p-complete", "name": "#p-complete", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "#P-complete problems are the most challenging problems within the #P class, and solving any one of them in polynomial time would imply that all #P problems can be solved in polynomial time. This classification is crucial for understanding the landscape of computational complexity."}, {"id": "fischer and rabin", "name": "fischer and rabin", "frequency": 1, "degree": 0, "layer": "tertiary", "evidence": "Fischer and Rabin are notable figures in computational theory, particularly for their work on the complexity of Presburger arithmetic. Their findings contribute to our understanding of the limits of algorithmic decision-making in mathematical logic."}, {"id": "chess strategy problem", "name": "chess strategy problem", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "The chess strategy problem exemplifies an EXPTIME-complete problem, illustrating the complexity involved in determining optimal moves in strategic games. Such examples are important for understanding the practical implications of computational complexity in game theory."}, {"id": "np-intermediate problems", "name": "np-intermediate problems", "frequency": 1, "degree": 2, "layer": "priority", "evidence": "NP-intermediate problems are those that exist in NP but are neither in P nor NP-complete, assuming P \u2260 NP. They are significant because they represent a class of problems whose complexity is not fully understood, making them crucial for theoretical computer science."}, {"id": "graph isomorphism problem", "name": "graph isomorphism problem", "frequency": 1, "degree": 4, "layer": "priority", "evidence": "The graph isomorphism problem involves checking if two graphs can be transformed into each other by renaming vertices. It is important because its classification within complexity theory (whether it is in P, NP-complete, or NP-intermediate) has implications for the understanding of computational complexity."}, {"id": "integer factorization problem", "name": "integer factorization problem", "frequency": 1, "degree": 7, "layer": "priority", "evidence": "This problem involves breaking down an integer into its prime components and is foundational in cryptography, particularly in systems like RSA. Its complexity status affects the security of many encryption methods."}, {"id": "polynomial time hierarchy", "name": "polynomial time hierarchy", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "The polynomial time hierarchy is a framework that classifies problems based on their complexity and the resources required to solve them. Understanding its structure is crucial for theoretical computer science, as it helps in analyzing the relationships between different complexity classes."}, {"id": "l\u00e1szl\u00f3 babai", "name": "l\u00e1szl\u00f3 babai", "frequency": 1, "degree": 0, "layer": "tertiary", "evidence": "L\u00e1szl\u00f3 Babai is a prominent computer scientist known for his contributions to complexity theory, particularly regarding the graph isomorphism problem. His work is significant as it provides insights into efficient algorithms for complex problems."}, {"id": "rsa algorithm", "name": "rsa algorithm", "frequency": 1, "degree": 0, "layer": "tertiary", "evidence": "The RSA algorithm is a widely used encryption method that relies on the difficulty of the integer factorization problem. Its security is contingent upon the assumption that no efficient algorithm exists for factoring large integers, making the integer factorization problem critical in cryptography."}, {"id": "shor's algorithm", "name": "shor's algorithm", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "Shor's algorithm is a quantum computing algorithm that can factor integers efficiently, which poses a potential threat to classical cryptographic systems. Its existence highlights the differences between quantum and classical complexity classes."}, {"id": "general number field sieve", "name": "general number field sieve", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "The general number field sieve is the best classical algorithm for factoring large integers, and its efficiency is crucial for understanding the practical implications of the integer factorization problem in cryptography."}, {"id": "cobham's thesis", "name": "cobham's thesis", "frequency": 1, "degree": 1, "layer": "priority", "evidence": "Cobham's thesis is a foundational concept in complexity theory that posits a distinction between problems that can be solved in polynomial time (P) and those that cannot (not in P). It is significant because it underpins much of the theoretical framework for classifying computational problems based on their inherent difficulty."}, {"id": "polynomial algorithm", "name": "polynomial algorithm", "frequency": 1, "degree": 1, "layer": "priority", "evidence": "A polynomial algorithm is one whose running time can be expressed as a polynomial function of the size of the input. While these algorithms are theoretically efficient, their practical applicability can be limited by large constants or exponents that affect performance, highlighting the gap between theoretical and practical complexity."}, {"id": "empirical average-case complexity", "name": "empirical average-case complexity", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "Empirical average-case complexity refers to the observed performance of an algorithm on typical instances of a problem, rather than the worst-case scenario. This concept is important because it highlights that algorithms can perform well in practice, even if their theoretical worst-case performance is poor."}, {"id": "simplex algorithm", "name": "simplex algorithm", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "The simplex algorithm is a widely used method for solving linear programming problems. Despite its exponential worst-case time complexity, it often performs efficiently on real-world problems, illustrating the difference between theoretical analysis and practical performance."}, {"id": "quantum computation", "name": "quantum computation", "frequency": 1, "degree": 6, "layer": "secondary", "evidence": "Quantum computation is a model of computation that utilizes quantum-mechanical phenomena to perform operations on data. It is significant because it challenges traditional notions of computational complexity and introduces new paradigms for problem-solving that may not fit within the classical P vs NP framework."}, {"id": "randomized algorithms", "name": "randomized algorithms", "frequency": 1, "degree": 2, "layer": "secondary", "evidence": "Randomized algorithms use random numbers to influence their behavior and can provide solutions to problems more efficiently than deterministic algorithms in some cases. They are important in complexity theory as they expand the landscape of computational methods beyond traditional models."}, {"id": "knuth's up-arrow notation", "name": "knuth's up-arrow notation", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "Knuth's up-arrow notation is a way to represent very large integers and is used to express operations that grow faster than exponential functions. It is relevant in complexity theory for illustrating the rapid growth of certain constants in algorithm analysis."}, {"id": "traveling salesman problem", "name": "traveling salesman problem", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "The traveling salesman problem is a classic NP-complete problem that seeks the shortest possible route visiting a set of cities and returning to the origin city. It serves as a benchmark for evaluating the performance of algorithms designed to tackle NP-complete challenges."}, {"id": "linear time on a multitape turing machine", "name": "linear time on a multitape turing machine", "frequency": 1, "degree": 2, "layer": "priority", "evidence": "Linear time on a multitape Turing machine refers to the computational complexity class where the time taken to solve a problem grows linearly with the size of the input. This concept is significant because it helps define the boundaries of efficient computation and is used to categorize problems based on their solvability within certain time constraints."}, {"id": "classes dlin and nlin", "name": "classes dlin and nlin", "frequency": 1, "degree": 2, "layer": "priority", "evidence": "DLIN and NLIN are complexity classes that arise from the substitution of linear time for polynomial time in the definitions of P and NP. Understanding these classes is important for analyzing the computational limits of algorithms and the relationships between different complexity classes."}, {"id": "dlin \u2260 nlin", "name": "dlin \u2260 nlin", "frequency": 1, "degree": 1, "layer": "priority", "evidence": "The statement DLIN \u2260 NLIN indicates that the class of problems solvable in linear time on a multitape Turing machine is not equivalent to the class of problems solvable in non-deterministic linear time. This distinction is significant in theoretical computer science as it suggests different levels of computational difficulty and has implications for understanding the nature of problem-solving in algorithms."}, {"id": "cryptography", "name": "cryptography", "frequency": 1, "degree": 4, "layer": "priority", "evidence": "Cryptography is the practice of securing communication and information through the use of codes. It relies on the difficulty of certain mathematical problems to ensure security. If P = NP, many cryptographic systems could be compromised, necessitating new security measures."}, {"id": "constructive proof", "name": "constructive proof", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "A constructive proof provides a method to actually find a solution, as opposed to merely demonstrating that a solution exists. This is important in computational theory because it not only confirms the existence of a solution but also offers a practical way to achieve it, which is crucial for real-world applications."}, {"id": "operations research", "name": "operations research", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "Operations research is a discipline that uses advanced analytical methods to help make better decisions. The presence of NP-complete problems in this field indicates that many logistical and optimization challenges remain computationally difficult, and efficient solutions could lead to significant advancements in various industries."}, {"id": "g\u00f6del's thoughts on computational complexity", "name": "g\u00f6del's thoughts on computational complexity", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "Kurt G\u00f6del was a mathematician known for his work on incompleteness theorems and computational complexity. His insights suggest that if a machine could solve all problems efficiently, it would fundamentally change the nature of mathematical inquiry and proof, highlighting the philosophical implications of computational capabilities."}, {"id": "fermat's last theorem", "name": "fermat's last theorem", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "Fermat's Last Theorem is a famous problem in number theory that remained unsolved for over 350 years until Andrew Wiles proved it in 1994. This example illustrates the challenges faced in mathematical proofs and the potential impact of efficient algorithms on the field."}, {"id": "donald knuth", "name": "donald knuth", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "Donald Knuth is a renowned computer scientist known for his work in algorithms and typesetting. His cautious stance on the implications of P = NP reflects the complexity and uncertainty surrounding the problem, emphasizing the need for careful consideration of its potential consequences."}, {"id": "average-case complexity", "name": "average-case complexity", "frequency": 1, "degree": 2, "layer": "secondary", "evidence": "Average-case complexity refers to the expected performance of an algorithm across all possible inputs, rather than its worst-case scenario. This concept is important because it can reveal that while a problem may be hard in the worst case, it could still be efficiently solvable for most practical instances."}, {"id": "sat problem", "name": "sat problem", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "The SAT (satisfiability) problem is a classic problem in computer science that asks whether there exists an assignment of variables that makes a given Boolean formula true. It is significant because it was the first problem proven to be NP-complete, serving as a benchmark for other problems in the NP class."}, {"id": "russell impagliazzo", "name": "russell impagliazzo", "frequency": 1, "degree": 0, "layer": "tertiary", "evidence": "Russell Impagliazzo is a prominent computer scientist known for his work on computational complexity and cryptography. His exploration of hypothetical 'worlds' provides a framework for understanding the implications of different outcomes in the P vs NP debate, which is crucial for guiding future research directions."}, {"id": "algorithmica", "name": "algorithmica", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "'Algorithmica' is one of the hypothetical scenarios proposed by Impagliazzo, where all problems in NP can be solved efficiently. This scenario represents an ideal outcome for computational problem-solving, highlighting the potential for significant advancements in algorithms and efficiency."}, {"id": "cryptomania", "name": "cryptomania", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "'Cryptomania' is another hypothetical scenario where P \u2260 NP holds true, and it emphasizes the ease of generating hard instances of problems. This scenario underscores the implications for cryptography and security, as it suggests that certain problems remain difficult to solve, preserving the security of cryptographic systems."}, {"id": "heuristica", "name": "heuristica", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "'Heuristica' represents a scenario where, despite P \u2260 NP, problems in NP can still be solved efficiently on average. This concept is important as it suggests that practical solutions may exist for many problems, even if they are not solvable in the worst case, influencing how researchers approach problem-solving."}, {"id": "princeton university workshop 2009", "name": "princeton university workshop 2009", "frequency": 1, "degree": 0, "layer": "tertiary", "evidence": "The 2009 workshop at Princeton University focused on the implications of the P vs NP question and the various hypothetical scenarios proposed by Impagliazzo. Such gatherings are crucial for fostering collaboration and advancing understanding in the field of computational complexity."}, {"id": "proof techniques", "name": "proof techniques", "frequency": 1, "degree": 4, "layer": "secondary", "evidence": "Proof techniques in computational complexity theory are methods used to demonstrate the relationships and properties of computational problems. Understanding these techniques is essential for researchers as they explore the boundaries of what can be proven regarding the P = NP problem."}, {"id": "independence result", "name": "independence result", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "An independence result refers to a situation where a statement cannot be proven or disproven within a given axiomatic system, such as Zermelo-Fraenkel set theory with the Axiom of Choice (ZFC). This concept is important in understanding the limitations of formal systems in addressing fundamental questions in mathematics and computer science."}, {"id": "zfc", "name": "zfc", "frequency": 1, "degree": 0, "layer": "tertiary", "evidence": "ZFC, or Zermelo-Fraenkel set theory with the Axiom of Choice, is a foundational system for mathematics. It is relevant to the P = NP problem because discussions about the provability of P = NP often involve whether the problem can be resolved within this framework."}, {"id": "peano axioms", "name": "peano axioms", "frequency": 1, "degree": 0, "layer": "tertiary", "evidence": "The Peano axioms are a set of axioms for the natural numbers that form the basis for arithmetic. Their extension is relevant in discussions about the decidability of problems in computational complexity, particularly in relation to the P = NP problem."}, {"id": "descriptive complexity", "name": "descriptive complexity", "frequency": 1, "degree": 1, "layer": "priority", "evidence": "Descriptive complexity is a branch of computational complexity theory that characterizes complexity classes in terms of the expressiveness of logical languages. It helps in understanding the relationship between computational problems and the logical frameworks used to describe them."}, {"id": "first-order logic", "name": "first-order logic", "frequency": 1, "degree": 1, "layer": "priority", "evidence": "First-order logic is a formal logical system that allows quantification over individual variables but not over predicates or functions. It is significant in computer science for its role in defining computational problems and understanding their complexity."}, {"id": "least fixed-point combinator", "name": "least fixed-point combinator", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "A least fixed-point combinator is a mathematical construct that allows the definition of recursive functions in a logical framework. It is crucial for expressing certain computational problems within first-order logic, particularly in the context of the P = NP problem."}, {"id": "recursive functions", "name": "recursive functions", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "Recursive functions are functions that can be defined in terms of themselves, allowing for the construction of complex algorithms. They are essential in computer science for modeling computation and understanding the limits of what can be computed."}, {"id": "existential second-order logic", "name": "existential second-order logic", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "Existential second-order logic is a logical framework that allows quantification over sets and relations but restricts universal quantification. It is important for characterizing the complexity class NP and understanding the expressiveness of different logical systems."}, {"id": "polynomial hierarchy (ph)", "name": "polynomial hierarchy (ph)", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "The polynomial hierarchy is a hierarchy of complexity classes that generalizes NP and co-NP. It is significant in theoretical computer science as it helps to classify problems based on their computational complexity and relationships between different classes."}, {"id": "semi-algorithm", "name": "semi-algorithm", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "A semi-algorithm is a type of algorithm that may not provide an answer for all inputs, specifically running indefinitely for certain cases. This concept is important in understanding the behavior of algorithms in the context of decision problems."}, {"id": "levin's algorithm", "name": "levin's algorithm", "frequency": 1, "degree": 3, "layer": "secondary", "evidence": "Levin's algorithm is an example of a theoretical approach to solving NP-complete problems, specifically SUBSET-SUM. It illustrates the complexities involved in algorithm design and the implications of the P vs NP question."}, {"id": "input", "name": "input", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "In the context of algorithms, input refers to the data provided to the algorithm for processing. Understanding the nature of input is essential for analyzing algorithm performance and behavior."}, {"id": "output", "name": "output", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "Output refers to the result produced by an algorithm after processing the input. It is a critical aspect of algorithm design, as it determines the effectiveness of the solution provided."}, {"id": "decision problem", "name": "decision problem", "frequency": 1, "degree": 3, "layer": "priority", "evidence": "A decision problem is a fundamental concept in computational theory that involves determining a binary outcome (yes or no) based on input strings. It is crucial for understanding how algorithms classify problems and their solvability within computational limits."}, {"id": "deterministic polynomial-time turing machine", "name": "deterministic polynomial-time turing machine", "frequency": 1, "degree": 0, "layer": "priority", "evidence": "A deterministic polynomial-time Turing machine is a theoretical model of computation that guarantees a solution in polynomial time for all inputs. This concept is critical for understanding the formal definitions of complexity classes in computational theory."}, {"id": "verifier", "name": "verifier", "frequency": 1, "degree": 2, "layer": "secondary", "evidence": "A verifier is an algorithm that checks whether a given solution (certificate) is valid for a decision problem. This concept is essential in the context of NP problems, where verification is often easier than finding a solution."}, {"id": "certificate", "name": "certificate", "frequency": 1, "degree": 3, "layer": "secondary", "evidence": "A certificate is a piece of information that helps verify whether a certain input belongs to a language in NP. Understanding certificates is crucial for grasping how NP problems can be efficiently checked even if they are hard to solve."}, {"id": "polynomial-time reduction", "name": "polynomial-time reduction", "frequency": 1, "degree": 2, "layer": "priority", "evidence": "Polynomial-time reduction is a method of transforming one problem into another in such a way that a solution to the second problem can be used to solve the first problem efficiently. This concept is essential in proving NP-completeness, as it establishes a relationship between problems and helps determine their relative difficulty."}, {"id": "finite alphabet \u03c3", "name": "finite alphabet \u03c3", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "A finite alphabet is a limited set of symbols from which strings (or languages) can be formed. In computational theory, the concept of a finite alphabet is foundational as it underpins the definition of languages and the problems associated with them, including NP-completeness."}, {"id": "common methods of proving np-completeness", "name": "common methods of proving np-completeness", "frequency": 1, "degree": 0, "layer": "tertiary", "evidence": "Common methods of proving NP-completeness often involve demonstrating that a known NP-complete problem can be polynomial-time reduced to the new problem. This approach is vital for establishing the computational complexity of new problems and understanding their place within the broader landscape of computational theory."}, {"id": "gerhard j. woeginger", "name": "gerhard j. woeginger", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "Gerhard J. Woeginger is a notable researcher in the field of computer science, particularly known for his work on the P versus NP problem. His compilation of purported proofs highlights the ongoing interest and debate surrounding this fundamental question."}, {"id": "purported proofs", "name": "purported proofs", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "Purported proofs refer to claims made by researchers that they have solved the P versus NP problem. These proofs are significant as they reflect the ongoing efforts and challenges in resolving one of the most critical questions in theoretical computer science."}, {"id": "media attention", "name": "media attention", "frequency": 1, "degree": 2, "layer": "tertiary", "evidence": "Media attention refers to the public and media coverage that certain claims or attempts to solve the P versus NP problem have received. This is important as it influences public perception and awareness of the complexities involved in such theoretical issues."}, {"id": "undecidable", "name": "undecidable", "frequency": 1, "degree": 0, "layer": "priority", "evidence": "The term 'undecidable' refers to problems for which no algorithm can be constructed that always leads to a correct yes-or-no answer. This concept is crucial in understanding the limits of computation and the boundaries of what can be solved within computer science."}, {"id": "refuted attempts", "name": "refuted attempts", "frequency": 1, "degree": 0, "layer": "secondary", "evidence": "Refuted attempts are claims or proofs that have been shown to be incorrect or invalid. Understanding these refutations is essential for grasping the complexities and challenges in proving or disproving the P versus NP problem."}, {"id": "116 purported proofs", "name": "116 purported proofs", "frequency": 1, "degree": 4, "layer": "tertiary", "evidence": "The 116 purported proofs represent various claims made by researchers regarding the P versus NP problem. This number illustrates the extensive interest and the multitude of approaches taken to tackle this significant question in computer science."}, {"id": "mathematicians", "name": "mathematicians", "frequency": 1, "degree": 3, "layer": "secondary", "evidence": "Mathematicians are professionals who specialize in the study of mathematics, including its theories, applications, and problem-solving techniques. In this context, they are crucial as they represent the intellectual effort to tackle one of the most significant challenges in theoretical computer science."}, {"id": "the simpsons", "name": "the simpsons", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "The Simpsons is an animated television series known for its satirical depiction of American life. Its inclusion of complex mathematical concepts like P = NP highlights the show's blend of humor and intellectual commentary, making advanced topics accessible to a broader audience."}, {"id": "elementary", "name": "elementary", "frequency": 1, "degree": 2, "layer": "tertiary", "evidence": "Elementary is a modern adaptation of Sherlock Holmes that incorporates contemporary themes and issues. The episode's focus on mathematicians and the P versus NP problem illustrates how popular media can engage with complex scientific ideas, raising awareness and interest in mathematical research."}, {"id": "us government", "name": "us government", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "The US government plays a role in funding and supporting research in various fields, including mathematics and computer science. Their involvement in the P versus NP problem underscores the significance of this issue for national security and technological advancement."}, {"id": "r vs. re problem", "name": "r vs. re problem", "frequency": 1, "degree": 3, "layer": "priority", "evidence": "The R vs. RE problem is a theoretical concept in computational complexity theory that compares two classes of problems: R, which is analogous to class P (problems solvable in polynomial time), and RE, which is analogous to class NP (nondeterministic polynomial time). Understanding this distinction is crucial for exploring the limits of what can be computed efficiently and what remains undecidable."}, {"id": "undecidable but verifiable problems", "name": "undecidable but verifiable problems", "frequency": 1, "degree": 2, "layer": "priority", "evidence": "Undecidable but verifiable problems are those for which no algorithm can determine a solution in all cases, yet if a solution is provided, it can be checked for correctness. This concept is important in understanding the limitations of computation and the nature of problems that can be addressed within the realms of R and RE."}, {"id": "hilbert's tenth problem", "name": "hilbert's tenth problem", "frequency": 1, "degree": 2, "layer": "secondary", "evidence": "Hilbert's tenth problem is a famous example of an undecidable problem that is RE-complete, meaning it is among the hardest problems in the class RE. This problem's significance lies in its implications for number theory and the limits of algorithmic solvability, making it a key example in discussions of computational complexity."}, {"id": "vp vs. vnp problem", "name": "vp vs. vnp problem", "frequency": 1, "degree": 2, "layer": "priority", "evidence": "The VP vs. VNP problem is a central question in algebraic complexity theory, analogous to the P vs. NP problem in classical complexity. It concerns the classification of problems based on their solvability and the efficiency of algorithms for polynomial equations, highlighting the complexity of algebraic computations."}, {"id": "unknown answer", "name": "unknown answer", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "The statement that the answer to the VP vs. VNP problem is currently unknown emphasizes the ongoing research and open questions in the field of computational complexity. This uncertainty reflects the broader challenges in understanding the limits of computation and the relationships between different complexity classes."}, {"id": "incompatible students", "name": "incompatible students", "frequency": 1, "degree": 0, "layer": "tertiary", "evidence": "In the context of the NP-problem example, incompatible students represent constraints that complicate the selection process. This illustrates how real-world problems can often be modeled as NP-problems, making them relevant for practical applications in scheduling and resource allocation."}, {"id": "william l. hosch", "name": "william l. hosch", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "William L. Hosch is an author who has contributed to the understanding of the P vs NP problem through his writings. His work helps disseminate complex mathematical concepts to a broader audience, making them more accessible."}, {"id": "example of housing accommodations problem", "name": "example of housing accommodations problem", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "This example illustrates a practical application of the P vs NP problem, where the challenge of selecting students for limited housing while adhering to constraints mirrors the complexities faced in NP-problems. It serves to contextualize theoretical concepts in real-world scenarios."}, {"id": "algorithms", "name": "algorithms", "frequency": 1, "degree": 1, "layer": "priority", "evidence": "Algorithms are step-by-step procedures or formulas for solving problems. They are fundamental to computer science and are used in various applications, from data processing to artificial intelligence. Understanding algorithms is crucial for developing efficient software and solving computational problems."}, {"id": "complexity classes", "name": "complexity classes", "frequency": 1, "degree": 1, "layer": "secondary", "evidence": "Complexity classes are categories used to classify computational problems based on their resource requirements, such as time and space. They help in understanding the efficiency of algorithms and the inherent difficulty of problems. This classification is essential for theoretical computer science and algorithm design."}, {"id": "isbn", "name": "isbn", "frequency": 1, "degree": 1, "layer": "tertiary", "evidence": "ISBN stands for International Standard Book Number, a unique identifier for books. It is used to simplify the distribution and purchase of books, ensuring that each title can be easily found and referenced. Understanding ISBNs is important for academic research and library sciences."}], "links": [{"source": "deterministic polynomial-time Turing machine", "target": "halting on all inputs and running in polynomial time", "type": "is characterized by", "evidence": "a deterministic polynomial-time Turing machine is a deterministic Turing machine M that satisfies two conditions: M halts on all inputs w and there exists k \u2208 N such that T_M(n) \u2208 O(n^k)."}, {"source": "cryptography", "target": "integer factorization problem", "type": "relies on the difficulty of", "evidence": "Many cryptographic systems, such as RSA, rely on the assumption that integer factorization is a hard problem, which is related to the broader discussions of computational complexity."}, {"source": "deterministic machine", "target": "non-deterministic machine", "type": "comparison with non-deterministic machine", "evidence": "The distinction between deterministic and non-deterministic machines is crucial for understanding the classes p and np, as it defines the capabilities and limitations of different computational models."}, {"source": "p versus np problem", "target": "millennium prize problems", "type": "is one of the", "evidence": "It is one of the seven Millennium Prize Problems selected by the Clay Mathematics Institute."}, {"source": "levin's algorithm", "target": "semi-algorithm", "type": "is an example of a", "evidence": "This is a polynomial-time algorithm accepting an NP-complete language only if P = NP. 'Accepting' means it gives 'yes' answers in polynomial time, but is allowed to run forever when the answer is 'no' (also known as a semi-algorithm)."}, {"source": "np-hard problems", "target": "NP problems", "type": "are at least as hard as", "evidence": "NP-hard problems are those at least as hard as NP problems."}, {"source": "Peano axioms", "target": "ZFC", "type": "are weaker than", "evidence": "if the problem is undecidable even with much weaker assumptions extending the Peano axioms"}, {"source": "integer factorization problem", "target": "general number field sieve", "type": "most efficient algorithm is", "evidence": "The most efficient known algorithm for integer factorization is the general number field sieve."}, {"source": "Russell Impagliazzo", "target": "five hypothetical 'worlds'", "type": "described", "evidence": "Russell Impagliazzo has described five hypothetical 'worlds' that could result from different possible resolutions to the average-case complexity question."}, {"source": "millennium prize problems", "target": "p versus np problem", "type": "significance in mathematics", "evidence": "The p versus np problem is one of the millennium prize problems, highlighting its importance and the potential implications for various fields if resolved."}, {"source": "historical context of np-completeness", "target": "theoretical computer science", "type": "provides insight into", "evidence": "Understanding the historical development of np-completeness helps to contextualize current research and theories in theoretical computer science, illustrating how past discoveries shape present inquiries."}, {"source": "confidence that P \u2260 NP", "target": "poll results on p vs np", "type": "increased", "evidence": "Confidence that P \u2260 NP has been increasing \u2013 in 2019, 88% believed P \u2260 NP."}, {"source": "Boolean satisfiability problem", "target": "NP-complete problems", "type": "is one of many", "evidence": "The Boolean satisfiability problem is one of many NP-complete problems."}, {"source": "np-complete problems", "target": "traveling salesman problem", "type": "examples include", "evidence": "There are algorithms for many NP-complete problems, such as the knapsack problem, the traveling salesman problem, and the Boolean satisfiability problem."}, {"source": "cook\u2013levin theorem", "target": "boolean satisfiability problem", "type": "foundational result", "evidence": "The cook\u2013levin theorem demonstrates that the boolean satisfiability problem is np-complete, serving as a cornerstone for the study of computational complexity and influencing subsequent research on other np-complete problems."}, {"source": "p versus np problem", "target": "millennium prize problems", "type": "is a type of", "evidence": "This problem concerns the issue of whether questions that are easy to verify (a class of queries called NP) also have solutions that are easy to find (a class called P)."}, {"source": "big o notation", "target": "knuth's up-arrow notation", "type": "describes complexity", "evidence": "The big O notation hides a constant that depends superexponentially on H."}, {"source": "cobham's thesis", "target": "np-complete problems", "type": "assumption about complexity", "evidence": "All of the above discussion has assumed that P means 'easy' and 'not in P' means 'difficult', an assumption known as Cobham's thesis."}, {"source": "Boolean satisfiability problem", "target": "Cook\u2013Levin theorem", "type": "is NP-complete by", "evidence": "the Boolean satisfiability problem is NP-complete by the Cook\u2013Levin theorem."}, {"source": "elementary", "target": "mathematicians", "type": "features", "evidence": "In the second episode of season 2 of Elementary, 'Solve for X' Sherlock and Watson investigate the murders of mathematicians who were attempting to solve P versus NP."}, {"source": "p versus np problem", "target": "mathematicians", "type": "is a problem addressed by", "evidence": "The film Travelling Salesman... is the story of four mathematicians hired by the US government to solve the P versus NP problem."}, {"source": "p \u2260 np", "target": "computational complexity", "type": "implications for", "evidence": "The assertion that p does not equal np has profound implications for computational complexity, as it suggests that certain problems cannot be solved efficiently, affecting various fields reliant on computational methods."}, {"source": "r vs. re problem", "target": "class p", "type": "is analogous to", "evidence": "R is analog of class P"}, {"source": "class P", "target": "class NP", "type": "comparison with", "evidence": "The relationship between class P and class NP is central to the discussion of NP-completeness."}, {"source": "william l. hosch", "target": "p versus np problem", "type": "has written about", "evidence": "Hosch, William L (11 August 2009). 'P versus NP problem mathematics'."}, {"source": "quickly verified", "target": "class np", "type": "is a characteristic of problems in", "evidence": "the class of questions where an answer can be verified in polynomial time is 'NP'."}, {"source": "np-intermediate problems", "target": "integer factorization problem", "type": "examples of", "evidence": "The graph isomorphism problem, the discrete logarithm problem, and the integer factorization problem are examples of problems believed to be NP-intermediate."}, {"source": "polynomial hierarchy (PH)", "target": "all of second-order logic", "type": "correspondence to second-order logic", "evidence": "The languages in the polynomial hierarchy, PH, correspond to all of second-order logic."}, {"source": "r vs. re problem", "target": "class np", "type": "is analogous to", "evidence": "RE is analog class NP"}, {"source": "verifier", "target": "certificate", "type": "is associated with", "evidence": "A Turing machine that decides LR is called a verifier for L and a y such that (x, y) \u2208 R is called a certificate of membership of x in L."}, {"source": "example of housing accommodations problem", "target": "np", "type": "is an example of", "evidence": "To complicate matters, the Dean has provided you with a list of pairs of incompatible students, and requested that no pair from this list appear in your final choice. This is an example of what computer scientists call an NP-problem."}, {"source": "Heuristica", "target": "P \u2260 NP but all problems in NP are tractable in the average case", "type": "is a world where", "evidence": "The 'world' where P \u2260 NP but all problems in NP are tractable in the average case is called 'Heuristica' in the paper."}, {"source": "gerhard j. woeginger", "target": "116 purported proofs", "type": "compiled", "evidence": "Gerhard J. Woeginger compiled a list of 116 purported proofs from 1986 to 2016"}, {"source": "116 purported proofs", "target": "6 proved other results", "type": "includes", "evidence": "and 6 proved other results, e.g. that the problem is undecidable"}, {"source": "class p", "target": "class np", "type": "subset of complexity classes", "evidence": "Class p is a subset of class np, indicating that problems that can be solved in polynomial time can also be verified in polynomial time, which is a foundational concept in computational complexity theory."}, {"source": "computational complexity", "target": "class p", "type": "is related to", "evidence": "the discussion of polynomial-time algorithms relates to computational complexity"}, {"source": "quantum computation", "target": "p versus np problem", "type": "may provide insights into", "evidence": "Quantum computation introduces new paradigms for solving problems, which could potentially lead to breakthroughs in understanding the p versus np problem."}, {"source": "algorithm research", "target": "np-complete problems", "type": "is concerned with", "evidence": "However, there are algorithms known for NP-complete problems..."}, {"source": "np-completeness", "target": "computational complexity", "type": "is a concept within", "evidence": "NP-completeness is a key concept that arises in the study of computational complexity."}, {"source": "3-sat", "target": "np-complete problems", "type": "example of NP-complete problem", "evidence": "A constructive and efficient solution to an NP-complete problem such as 3-SAT would break most existing cryptosystems."}, {"source": "Presburger arithmetic", "target": "EXPTIME-complete problems", "type": "requires more time than", "evidence": "The problem of deciding the truth of a statement in Presburger arithmetic requires even more time."}, {"source": "p versus np problem", "target": "stephen cook", "type": "introduced by", "evidence": "The precise statement of the P versus NP problem was introduced in 1971 by Stephen Cook in his seminal paper."}, {"source": "p versus np problem", "target": "polynomial time", "type": "involves verification in", "evidence": "since a proposed key can be verified in polynomial time."}, {"source": "poll results on p vs np", "target": "subjective opinion of this era", "type": "reflects", "evidence": "These polls do not imply whether P = NP... but it attempts to be an objective report on the subjective opinion of this era."}, {"source": "automated discovery of mathematical proofs", "target": "algorithm research", "type": "influenced by", "evidence": "The automated discovery of mathematical proofs is influenced by advancements in algorithm research, particularly in the context of solving complex problems like those in class NP."}, {"source": "cook\u2013levin theorem", "target": "boolean satisfiability problem and np-complete problems", "type": "establishes the equivalence of", "evidence": "The Cook-Levin theorem shows that the boolean satisfiability problem is NP-complete, which is a pivotal result in understanding the nature of NP-completeness."}, {"source": "the simpsons", "target": "p versus np problem", "type": "references", "evidence": "In the sixth episode of The Simpsons' seventh season 'Treehouse of Horror VI', the equation P = NP is seen..."}, {"source": "table lookup", "target": "class p", "type": "is a method for solving", "evidence": "the answer can be found by table lookup"}, {"source": "proof by reduction", "target": "many other problems are NP-complete", "type": "provides a method to show", "evidence": "proof by reduction provided a simpler way to show that many other problems are also NP-complete."}, {"source": "vp vs. vnp problem", "target": "r vs. re problem", "type": "is similar to", "evidence": "A similar problem exists in the theory of algebraic complexity: VP vs. VNP problem"}, {"source": "Turing machine", "target": "NP", "type": "is used to define", "evidence": "a trivial NP-complete problem can be formulated as follows: given a Turing machine M guaranteed to halt in polynomial time."}, {"source": "clay mathematics institute", "target": "millennium prize problems", "type": "selected the", "evidence": "It is one of the seven Millennium Prize Problems selected by the Clay Mathematics Institute."}, {"source": "operations research", "target": "np-complete problems", "type": "field impacted by NP-complete problems", "evidence": "Many problems in operations research are NP-complete, such as types of integer programming and the travelling salesman problem."}, {"source": "p = np", "target": "algorithm research", "type": "is a hypothesis that impacts", "evidence": "If p equals np, it would revolutionize algorithm research by allowing efficient solutions to many currently intractable problems."}, {"source": "3-SAT", "target": "Boolean satisfiability", "type": "is a special case of", "evidence": "which could then be used to find solutions for the special case of SAT known as 3-SAT."}, {"source": "p = np", "target": "polynomial time", "type": "condition for", "evidence": "it runs in polynomial time on inputs that are in SUBSET-SUM if and only if P = NP."}, {"source": "william gasarch", "target": "poll results on p vs np", "type": "conducted", "evidence": "Since 2002, William Gasarch has conducted three polls of researchers concerning this and related questions."}, {"source": "p versus np problem", "target": "EXPTIME", "type": "is related to", "evidence": "Although it is unknown whether P = NP, problems outside of P are known."}, {"source": "cook\u2013levin theorem", "target": "boolean satisfiability problem", "type": "establishes a connection between", "evidence": "The cook\u2013levin theorem shows that the boolean satisfiability problem is np-complete, thus linking it to the broader class of np-complete problems."}, {"source": "Algorithmica", "target": "P = NP", "type": "represents", "evidence": "Algorithmica, where P = NP and problems like SAT can be solved efficiently in all instances."}, {"source": "116 purported proofs", "target": "49 proofs of P \u2260 NP", "type": "includes", "evidence": "49 were proofs of P \u2260 NP"}, {"source": "co-np-complete", "target": "class np", "type": "related to", "evidence": "Co-np-complete problems are related to class NP, as they represent the complement of NP problems, further illustrating the complexity landscape in theoretical computer science."}, {"source": "clay mathematics institute", "target": "millennium prize problems", "type": "established", "evidence": "The Clay Mathematics Institute established the millennium prize problems to incentivize research in critical areas of mathematics, including the p versus np problem."}, {"source": "polynomial time", "target": "class p", "type": "is the time complexity for problems in", "evidence": "the general class of questions that some algorithm can answer in polynomial time is 'P' or 'class P'."}, {"source": "independence result", "target": "P \u2260 NP", "type": "could imply that", "evidence": "an independence result could imply that either P \u2260 NP and this is unprovable in ZFC"}, {"source": "isbn", "target": "books on computational complexity", "type": "is a reference for", "evidence": "The ISBNs listed correspond to significant texts in the field of computational complexity."}, {"source": "decision problem", "target": "computational complexity", "type": "is a fundamental concept in", "evidence": "The decision problem serves as a basis for classifying problems within computational complexity, influencing the understanding of classes like p and np."}, {"source": "computational complexity theory", "target": "complexity classes p and np", "type": "studies", "evidence": "The relation between the complexity classes P and NP is studied in computational complexity theory."}, {"source": "decision problem", "target": "a problem that takes as input some string w over an alphabet \u03a3, and outputs 'yes' or 'no'", "type": "is defined as", "evidence": "A decision problem is a problem that takes as input some string w over an alphabet \u03a3, and outputs 'yes' or 'no'."}, {"source": "EXPTIME", "target": "set of all decision problems with exponential running time", "type": "is defined as", "evidence": "the class EXPTIME is the set of all decision problems that have exponential running time."}, {"source": "exponential time", "target": "polynomial time", "type": "contrasts with", "evidence": "Exponential time algorithms are significantly less efficient than polynomial time algorithms, which are central to the P vs NP discussion."}, {"source": "cryptography", "target": "integer factorization problem", "type": "relies on the difficulty of solving", "evidence": "Many cryptographic systems, such as RSA, depend on the assumption that integer factorization is hard, which is related to the broader discussions of complexity classes and the p versus np problem."}, {"source": "average-case complexity", "target": "algorithm research", "type": "is a consideration in", "evidence": "Research in algorithms often takes into account average-case complexity to better understand the performance of algorithms beyond worst-case scenarios."}, {"source": "quantum computation", "target": "p and np", "type": "type of computation", "evidence": "Finally, there are types of computations which do not conform to the Turing machine model on which P and NP are defined, such as quantum computation and randomized algorithms."}, {"source": "p = np", "target": "np-complete problems", "type": "impact on NP-complete problems", "evidence": "A proof that P = NP could have stunning practical consequences if the proof leads to efficient methods for solving some of the important problems in NP."}, {"source": "big O notation", "target": "the upper bound of the time complexity of algorithms", "type": "is used to describe", "evidence": "where O refers to the big O notation."}, {"source": "class NP", "target": "NP-complete problems", "type": "includes", "evidence": "L is NP-complete if, and only if, L \u2208 NP."}, {"source": "complexity classes", "target": "computational complexity", "type": "are defined by", "evidence": "Complexity classes are a fundamental aspect of the study of computational complexity."}, {"source": "these attempts", "target": "refuted", "type": "have been", "evidence": "though these attempts have been refuted"}, {"source": "generalized sudoku problem", "target": "class np", "type": "is in", "evidence": "generalized Sudoku is in NP (quickly verifiable)"}, {"source": "stephen cook", "target": "theoretical computer science", "type": "contributor to", "evidence": "Stephen Cook is a pivotal figure in theoretical computer science, known for formulating the p versus np problem, which is foundational to the field."}, {"source": "p versus np problem", "target": "unsolved", "type": "is considered", "evidence": "While the P versus NP problem is generally considered unsolved"}, {"source": "generalized sudoku problem", "target": "class np", "type": "example of", "evidence": "The generalized sudoku problem serves as an example of a problem in class NP, as it can be verified quickly, illustrating the characteristics of NP problems."}, {"source": "non-deterministic machine", "target": "class NP", "type": "is used in the definition of", "evidence": "NP can be defined similarly using nondeterministic Turing machines."}, {"source": "p \u2260 np", "target": "computational complexity theory", "type": "is a central problem in", "evidence": "the P = NP problem itself remains open despite a million-dollar prize and a huge amount of dedicated research"}, {"source": "finite number of possible grids", "target": "class p", "type": "implies", "evidence": "any fixed size Sudoku has only a finite number of possible grids. In this case the problem is in P"}, {"source": "p versus np problem", "target": "computational complexity", "type": "challenges understanding of", "evidence": "The P vs NP problem is a central question in computational complexity theory, questioning whether every problem whose solution can be quickly verified can also be quickly solved."}, {"source": "finite alphabet \u03a3", "target": "language L", "type": "context for", "evidence": "Let L be a language over a finite alphabet \u03a3."}, {"source": "Sudoku", "target": "Latin squares", "type": "is reducible to", "evidence": "the proof shows that a solution of Sudoku in polynomial time could also be used to complete Latin squares in polynomial time."}, {"source": "polynomial time", "target": "linear time on a multitape turing machine", "type": "substituted for", "evidence": "When one substitutes 'linear time on a multitape Turing machine' for 'polynomial time' in the definitions of P and NP, one obtains the classes DLIN and NLIN."}, {"source": "time hierarchy theorem", "target": "EXPTIME-complete problems cannot be solved in significantly less than exponential time", "type": "implies that", "evidence": "by the time hierarchy theorem, they cannot be solved in significantly less than exponential time."}, {"source": "historical context of np-completeness", "target": "current computational complexity challenges", "type": "provides background for understanding", "evidence": "Understanding the historical context of np-completeness helps frame the ongoing challenges and research directions in computational complexity."}, {"source": "vp vs. vnp problem", "target": "unknown answer", "type": "has unknown answer", "evidence": "Like P vs. NP, the answer is currently unknown"}, {"source": "historical context of np-completeness", "target": "stephen cook", "type": "development of theoretical concepts", "evidence": "Stephen Cook's work laid the foundation for the historical context of np-completeness, influencing the evolution of theoretical computer science and the understanding of computational problems."}, {"source": "quantum computation", "target": "p versus np problem", "type": "challenges traditional views on", "evidence": "Quantum algorithms, such as Shor's algorithm, suggest that certain problems may be solvable more efficiently than previously thought, prompting reevaluation of the p versus np question."}, {"source": "p versus np problem", "target": "leonid levin", "type": "introduced independently by", "evidence": "and independently by Leonid Levin in 1973."}, {"source": "p \u2260 np", "target": "partial solutions or solutions to other problems", "type": "focuses research on", "evidence": "It would demonstrate that many common problems cannot be solved efficiently, so that the attention of researchers can be focused on partial solutions or solutions to other problems."}, {"source": "historical context of np-completeness", "target": "p = np", "type": "is relevant to understanding", "evidence": "The historical context of NP-completeness is important in discussions about P = NP."}, {"source": "exponential time", "target": "p \u2260 np", "type": "implies", "evidence": "If proved (and Nash was suitably skeptical), this would imply what is now called P \u2260 NP."}, {"source": "proof techniques", "target": "np-completeness", "type": "are essential for establishing", "evidence": "Various proof techniques, such as polynomial-time reductions, are crucial for demonstrating that a problem is np-complete, linking the methods of proof to the classification of problems."}, {"source": "p versus np problem", "target": "theoretical computer science", "type": "is a major unsolved problem in", "evidence": "The P versus NP problem is a major unsolved problem in theoretical computer science."}, {"source": "p versus np problem", "target": "computational complexity theory", "type": "is a central question in", "evidence": "The p versus np problem is foundational in computational complexity theory, as it questions whether every problem whose solution can be quickly verified can also be quickly solved."}, {"source": "undecidable but verifiable problems", "target": "hilbert's tenth problem", "type": "includes example of", "evidence": "for example, Hilbert's tenth problem which is RE-complete"}, {"source": "descriptive complexity", "target": "P = NP problem", "type": "context for logical statements", "evidence": "The P = NP problem can be restated as certain classes of logical statements, as a result of work in descriptive complexity."}, {"source": "empirical average-case complexity", "target": "simplex algorithm", "type": "describes performance", "evidence": "An example is the simplex algorithm in linear programming, which works surprisingly well in practice."}, {"source": "cook\u2013levin theorem", "target": "boolean satisfiability problem", "type": "establishes the equivalence of", "evidence": "The Cook-Levin theorem shows that the boolean satisfiability problem is NP-complete, linking it to the broader class of NP problems."}, {"source": "P versus NP problem", "target": "standard axiom systems like ZFC", "type": "may be independent of", "evidence": "some computer scientists suggest the P versus NP problem may be independent of standard axiom systems like ZFC"}, {"source": "proof techniques", "target": "np-completeness", "type": "are essential for understanding", "evidence": "Various proof techniques, such as proof by reduction, are crucial for establishing the np-completeness of problems, thereby linking them to the broader framework of computational complexity."}, {"source": "L is NP-complete", "target": "L \u2208 NP", "type": "definition", "evidence": "L is NP-complete if, and only if, the following two conditions are satisfied: L \u2208 NP"}, {"source": "NP", "target": "existential second-order logic", "type": "expressibility of languages", "evidence": "NP is the set of languages expressible in existential second-order logic."}, {"source": "input", "target": "levin's algorithm", "type": "is required for", "evidence": "Input: S = a finite set of integers"}, {"source": "proof techniques", "target": "np-completeness", "type": "are essential for demonstrating", "evidence": "Various proof techniques, such as proof by reduction, are fundamental in establishing the NP-completeness of problems, which is a key aspect of computational complexity theory."}, {"source": "first-order logic", "target": "languages in P", "type": "expressibility of languages in P", "evidence": "all such languages in P are expressible in first-order logic with the addition of a suitable least fixed-point combinator."}, {"source": "np-complete problems", "target": "any other problem in NP", "type": "are at least as tough as", "evidence": "Informally, an NP-complete problem is an NP problem that is at least as 'tough' as any other problem in NP."}, {"source": "exponential time", "target": "polynomial time", "type": "is contrasted with", "evidence": "The distinction between problems solvable in polynomial time versus those requiring exponential time is crucial in computational complexity, particularly in the context of NP-completeness."}, {"source": "L\u00e1szl\u00f3 Babai", "target": "graph isomorphism problem", "type": "developed algorithm for", "evidence": "The best algorithm for this problem, due to L\u00e1szl\u00f3 Babai, runs in quasi-polynomial time."}, {"source": "proofs of independence", "target": "current techniques", "type": "are impossible with", "evidence": "proofs of independence with those techniques are impossible"}, {"source": "graph isomorphism problem", "target": "polynomial time hierarchy", "type": "affects", "evidence": "If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level."}, {"source": "historical context of np-completeness", "target": "np-completeness", "type": "provides background for understanding", "evidence": "The historical context of np-completeness helps to understand its development and significance."}, {"source": "sudoku grid", "target": "generalized sudoku problem", "type": "example of computational problem", "evidence": "The sudoku grid serves as a specific instance of the generalized sudoku problem, illustrating practical applications of concepts in computational complexity and algorithm design."}, {"source": "classes dlin and nlin", "target": "dlin \u2260 nlin", "type": "not equal to", "evidence": "It is known that DLIN \u2260 NLIN."}, {"source": "average-case complexity", "target": "hard problems in NP", "type": "remains open in", "evidence": "P \u2260 NP still leaves open the average-case complexity of hard problems in NP."}, {"source": "kurt g\u00f6del", "target": "co-np-complete", "type": "inquired about", "evidence": "G\u00f6del asked whether theorem-proving (now known to be co-NP-complete) could be solved in quadratic or linear time."}, {"source": "complexity classes p and np", "target": "deterministic machine", "type": "defined by", "evidence": "The class P consists of all decision problems solvable on a deterministic sequential machine."}, {"source": "SAT", "target": "exponential time in the worst case", "type": "may require", "evidence": "it is possible that SAT requires exponential time in the worst case."}, {"source": "class p", "target": "class np", "type": "subset of", "evidence": "Class P consists of problems that can be solved in polynomial time, which is a subset of Class NP, where problems can be verified in polynomial time, highlighting the relationship between solvability and verifiability."}, {"source": "complexity classes p and np", "target": "non-deterministic machine", "type": "defined by", "evidence": "The class NP consists of all decision problems whose positive solutions are verifiable in polynomial time... on a non-deterministic machine."}, {"source": "fermat's last theorem", "target": "p = np", "type": "example of long-standing mathematical problem", "evidence": "A method guaranteed to find a proof if a 'reasonable' size proof exists, would essentially end this struggle."}, {"source": "graph isomorphism problem", "target": "not NP-complete", "type": "is believed to be", "evidence": "It is believed that the problem is at least not NP-complete."}, {"source": "p versus np problem", "target": "algorithm research", "type": "challenges", "evidence": "The p versus np problem is a central question in theoretical computer science that directly influences algorithm research, as it determines the feasibility of solving problems efficiently."}, {"source": "algorithm research", "target": "p versus np problem", "type": "is influenced by", "evidence": "The ongoing research into algorithms is heavily motivated by the implications of the P vs NP problem, as solving it could revolutionize algorithm efficiency."}, {"source": "np-complete problems", "target": "class np", "type": "are a subset of", "evidence": "NP-complete problems are defined as the hardest problems in NP, meaning that if any NP-complete problem can be solved in polynomial time, then all problems in NP can also be solved in polynomial time."}, {"source": "np-complete problems", "target": "polynomial time", "type": "has no known algorithm that runs in", "evidence": "No known algorithm for a NP-complete problem runs in polynomial time."}, {"source": "linear time on a multitape turing machine", "target": "classes dlin and nlin", "type": "defines", "evidence": "When one substitutes 'linear time on a multitape Turing machine' for 'polynomial time' in the definitions of P and NP, one obtains the classes DLIN and NLIN."}, {"source": "media attention", "target": "millennium prize problems", "type": "reflects public interest in", "evidence": "The media's coverage of the millennium prize problems, including the p versus np problem, highlights the significance and intrigue surrounding these unresolved questions in mathematics."}, {"source": "p \u2260 np", "target": "computational complexity theory", "type": "advances", "evidence": "A proof of P \u2260 NP would represent a great advance in computational complexity theory."}, {"source": "sudoku grid", "target": "generalized sudoku problem", "type": "is a type of", "evidence": "given an incomplete Sudoku grid of size n^2 \u00d7 n^2"}, {"source": "p versus np problem", "target": "computational complexity", "type": "is a central question in", "evidence": "The p versus np problem is a fundamental question in the field of computational complexity."}, {"source": "us government", "target": "mathematicians", "type": "hires", "evidence": "The film Travelling Salesman... is the story of four mathematicians hired by the US government to solve the P versus NP problem."}, {"source": "NP", "target": "co-NP", "type": "implication of co-NP", "evidence": "the former would establish that NP = co-NP."}, {"source": "cook\u2013levin theorem", "target": "np-completeness and polynomial-time verification", "type": "establishes the equivalence of", "evidence": "The cook\u2013levin theorem demonstrates that a problem is np-complete if it can be verified in polynomial time, linking the concepts of verification and computational difficulty."}, {"source": "randomized algorithms", "target": "solving complex problems", "type": "are a strategy for", "evidence": "Randomized algorithms provide alternative methods for tackling problems that may be intractable under deterministic approaches, relevant to NP-completeness."}, {"source": "hilbert's tenth problem", "target": "undecidable but verifiable problems", "type": "is classified as", "evidence": "Hilbert's tenth problem which is RE-complete"}, {"source": "sudoku", "target": "np-complete problems", "type": "is an example of", "evidence": "The generalized sudoku problem is NP-complete, illustrating the practical implications of NP-completeness in puzzle-solving."}, {"source": "P = NP", "target": "P = PH", "type": "equivalence to PH", "evidence": "P = NP if and only if P = PH."}, {"source": "decision problem", "target": "NP-completeness", "type": "related to", "evidence": "There are many equivalent ways of describing NP-completeness."}, {"source": "randomized algorithms", "target": "p and np", "type": "type of computation", "evidence": "Finally, there are types of computations which do not conform to the Turing machine model on which P and NP are defined, such as quantum computation and randomized algorithms."}, {"source": "np-complete problems", "target": "any other NP problem", "type": "are reducible to", "evidence": "NP-complete problems are problems that any other NP problem is reducible to in polynomial time."}, {"source": "np-complete problems", "target": "boolean satisfiability problem", "type": "examples include", "evidence": "There are algorithms for many NP-complete problems, such as the knapsack problem, the traveling salesman problem, and the Boolean satisfiability problem."}, {"source": "p \u2286 np", "target": "p \u2260 np", "type": "relationship", "evidence": "Clearly, P \u2286 NP. Arguably, the biggest open question in theoretical computer science concerns the relationship between those two classes: Is P equal to NP?"}, {"source": "millennium prize problems", "target": "p versus np problem", "type": "includes the", "evidence": "The p versus np problem is one of the seven Millennium Prize Problems, highlighting its significance in mathematics and computer science."}, {"source": "p = np", "target": "cryptography", "type": "potential consequence", "evidence": "A solution showing P = NP could upend the field of cryptography, which relies on certain problems being difficult."}, {"source": "john nash", "target": "exponential time", "type": "speculated about", "evidence": "In 1955, mathematician John Nash wrote a letter to the NSA, speculating that cracking a sufficiently complex code would require time exponential in the length of the key."}, {"source": "L is NP-complete", "target": "any L' in NP is polynomial-time-reducible to L", "type": "definition", "evidence": "L is NP-complete if, and only if, the following two conditions are satisfied: any L' in NP is polynomial-time-reducible to L"}, {"source": "co-np-complete", "target": "automated discovery of mathematical proofs", "type": "could lead to", "evidence": "pointed out one of the most important consequences\u2014that if so, then the discovery of mathematical proofs could be automated."}, {"source": "undecidable problems", "target": "EXPTIME-complete problems", "type": "are more difficult than", "evidence": "Even more difficult are the undecidable problems, such as the halting problem."}, {"source": "g\u00f6del's thoughts on computational complexity", "target": "p = np", "type": "historical perspective", "evidence": "G\u00f6del, in his early thoughts on computational complexity, noted that a mechanical method that could solve any problem would revolutionize mathematics."}, {"source": "constructive proof", "target": "p = np", "type": "type of proof relevant to p = np", "evidence": "A non-constructive proof might show a solution exists without specifying either an algorithm to obtain it or a specific bound."}, {"source": "least fixed-point combinator", "target": "recursive functions", "type": "definition tool for recursive functions", "evidence": "Recursive functions can be defined with this and the order relation."}, {"source": "polynomial-time reduction", "target": "NP-completeness", "type": "method of proving", "evidence": "Alternatively, if L \u2208 NP, and there is another NP-complete problem that can be polynomial-time reduced to L, then L is NP-complete."}, {"source": "integer factorization problem", "target": "np-hard problems", "type": "is related to", "evidence": "The integer factorization problem is considered NP-hard, indicating its difficulty and relevance in the context of computational complexity."}, {"source": "#P-complete", "target": "#P problems", "type": "is a subset of", "evidence": "Many of these problems are #P-complete, and hence among the hardest problems in #P."}, {"source": "output", "target": "levin's algorithm", "type": "is produced by", "evidence": "Output: 'yes' if any subset of S adds up to 0."}, {"source": "generalized sudoku problem", "target": "class p", "type": "may or may not be in", "evidence": "it is not known whether there is a polynomial-time algorithm that can correctly answer 'yes' or 'no' to all instances of this problem"}, {"source": "#P problems", "target": "NP problems", "type": "are at least as hard as", "evidence": "a #P problem must be at least as hard as the corresponding NP problem."}, {"source": "quantum computation", "target": "classical NP-complete problems", "type": "offers potential solutions to", "evidence": "Quantum algorithms, such as Shor's algorithm, suggest that quantum computation may provide efficient solutions to problems that are classically NP-complete, thus impacting the understanding of computational complexity."}, {"source": "116 purported proofs", "target": "61 proofs of P = NP", "type": "includes", "evidence": "of which 61 were proofs of P = NP"}, {"source": "polynomial-time solution to Sudoku", "target": "polynomial time solution of satisfiability", "type": "leads to", "evidence": "a polynomial-time solution to Sudoku leads, by a series of mechanical transformations, to a polynomial time solution of satisfiability."}, {"source": "time hierarchy theorem", "target": "complexity classes p and np", "type": "provides insights into", "evidence": "The time hierarchy theorem helps to understand the relationships and separations between different complexity classes, including P and NP."}, {"source": "np-complete problems", "target": "solving the P = NP problem", "type": "are useful for", "evidence": "if a polynomial-time algorithm can be demonstrated for an NP-complete problem, this would solve the P = NP problem"}, {"source": "p versus np problem", "target": "algorithm research", "type": "implication for research", "evidence": "The p versus np problem is central to algorithm research as it determines the feasibility of solving problems efficiently, influencing the direction of research in theoretical computer science."}, {"source": "chess strategy problem", "target": "EXPTIME-complete problems", "type": "is an example of", "evidence": "Examples include finding a perfect strategy for chess positions on an N \u00d7 N board."}, {"source": "Princeton University workshop 2009", "target": "the status of the five worlds", "type": "studied", "evidence": "A Princeton University workshop in 2009 studied the status of the five worlds."}, {"source": "cryptography", "target": "p versus np problem", "type": "is influenced by the outcomes of", "evidence": "The security of many cryptographic systems relies on the assumption that certain problems are hard to solve, which is directly related to the p versus np question."}, {"source": "algorithm research", "target": "np-hard problems", "type": "is driven by the need to solve", "evidence": "Research in algorithms often focuses on finding efficient solutions or approximations for NP-hard problems, which are critical in various applications."}, {"source": "proof techniques", "target": "p \u2260 np", "type": "are insufficient for proving", "evidence": "existing proof techniques are insufficient for answering the question"}, {"source": "certificate", "target": "membership in NP", "type": "is required for", "evidence": "for L to be in NP, there must be a verifier that runs in polynomial time."}, {"source": "quantum computation", "target": "p versus np problem", "type": "offers potential solutions to", "evidence": "Quantum computation may provide new approaches to solving NP problems more efficiently, suggesting a possible resolution to the P vs NP question."}, {"source": "verifier", "target": "certificate", "type": "associated with", "evidence": "A verifier checks the validity of a certificate for a decision problem."}, {"source": "elementary", "target": "p versus np problem", "type": "involves solving", "evidence": "In the second episode of season 2 of Elementary, 'Solve for X' Sherlock and Watson investigate the murders of mathematicians who were attempting to solve P versus NP."}, {"source": "EXPTIME-complete", "target": "EXPTIME", "type": "is a subset of", "evidence": "A decision problem is EXPTIME-complete if it is in EXPTIME."}, {"source": "integer factorization problem", "target": "Shor's algorithm", "type": "best known quantum algorithm is", "evidence": "The best known quantum algorithm for this problem, Shor's algorithm, runs in polynomial time."}, {"source": "donald knuth", "target": "p = np", "type": "opinion on p = np", "evidence": "Donald Knuth has stated that he has come to believe that P = NP, but is reserved about the impact of a possible proof."}, {"source": "theoretical computer science", "target": "computational complexity theory", "type": "field of study", "evidence": "Theoretical computer science encompasses computational complexity theory, which studies the classification of problems based on their inherent difficulty and the resources required to solve them."}, {"source": "polynomial-time Turing machine", "target": "polynomial-time reduction", "type": "required for", "evidence": "there exists a polynomial-time Turing machine that halts with f(w) on its tape on any input w."}, {"source": "exponential time", "target": "polynomial time", "type": "contrast with polynomial time", "evidence": "Exponential time algorithms are contrasted with polynomial time algorithms, emphasizing the efficiency gap that is central to the p versus np problem and its implications for algorithm research."}, {"source": "np", "target": "p versus np problem", "type": "is related to", "evidence": "This problem concerns the issue of whether questions that are easy to verify (a class of queries called NP) also have solutions that are easy to find (a class called P)."}, {"source": "algorithms", "target": "computational complexity", "type": "are used to solve problems in", "evidence": "Algorithms are essential for addressing problems categorized within computational complexity."}, {"source": "exponential time", "target": "polynomial time", "type": "contrast with", "evidence": "Exponential time algorithms are contrasted with polynomial time algorithms, as the former are generally impractical for large inputs, highlighting the importance of efficient algorithms in computational complexity."}, {"source": "p versus np problem", "target": "various fields", "type": "has implications for", "evidence": "a proof either way would have profound implications for mathematics, cryptography, algorithm research, artificial intelligence, game theory, multimedia processing, philosophy, economics and many other fields."}, {"source": "finite number of possible grids", "target": "decision problems in NP", "type": "is relevant to", "evidence": "The following defines a 'verifier': Let L be a language over a finite alphabet, \u03a3."}, {"source": "np-completeness", "target": "np-complete problems", "type": "relationship to problem difficulty", "evidence": "The concept of np-completeness categorizes certain problems as being as hard as the hardest problems in np, establishing a critical framework for understanding problem difficulty in computational complexity."}, {"source": "class P", "target": "languages that can be decided by a deterministic polynomial-time Turing machine", "type": "contains", "evidence": "we say that the problem can be solved in polynomial time and we place it in the class P."}, {"source": "some attempts at resolving P versus NP", "target": "media attention", "type": "received", "evidence": "Some attempts at resolving P versus NP have received brief media attention"}, {"source": "np-intermediate problems", "target": "graph isomorphism problem", "type": "examples of", "evidence": "The graph isomorphism problem, the discrete logarithm problem, and the integer factorization problem are examples of problems believed to be NP-intermediate."}, {"source": "millennium prize problems", "target": "p versus np problem", "type": "includes", "evidence": "The p versus np problem is one of the seven millennium prize problems established by the Clay Mathematics Institute, emphasizing its significance in mathematics and computer science."}, {"source": "np-complete problems", "target": "class np", "type": "are examples of", "evidence": "NP-complete problems are a subset of NP problems that are as hard as the hardest problems in NP, illustrating the complexity of this class."}, {"source": "historical context of np-completeness", "target": "computational complexity theory", "type": "provides background for understanding", "evidence": "Understanding the historical context of NP-completeness helps to frame the development of computational complexity theory and its key concepts."}, {"source": "Fischer and Rabin", "target": "Presburger arithmetic requires more than exponential run time", "type": "proved that", "evidence": "Fischer and Rabin proved in 1974 that every algorithm that decides the truth of Presburger statements of length n has a runtime of at least 2^{2^{cn}}."}, {"source": "class NP", "target": "the set of languages with a finite alphabet and verifier that runs in polynomial time", "type": "is defined as", "evidence": "NP can be defined similarly using nondeterministic Turing machines... Formally, NP is the set of languages with a finite alphabet and verifier that runs in polynomial time."}, {"source": "p \u2260 np", "target": "is P a proper subset of NP", "type": "reformulation of a question", "evidence": "the question 'is P a proper subset of NP' can be reformulated as 'is existential second-order logic able to describe languages that first-order logic with least fixed point cannot?'"}, {"source": "p \u2260 np", "target": "p versus np problem", "type": "is widely believed to be true", "evidence": "If P \u2260 NP, which is widely believed, it would mean that there are problems in NP that are harder to compute than to verify."}, {"source": "computational complexity", "target": "class np", "type": "is related to", "evidence": "the discussion of NP relates to computational complexity"}, {"source": "Cryptomania", "target": "P \u2260 NP", "type": "represents", "evidence": "Cryptomania, where P \u2260 NP and generating hard instances of problems outside P is easy."}, {"source": "quantum computation", "target": "classical computational problems", "type": "offers potential solutions to", "evidence": "Quantum algorithms, such as Shor's algorithm for integer factorization, suggest that quantum computation may provide efficient solutions to problems that are currently believed to be hard in classical settings."}, {"source": "np-completeness", "target": "P = NP question", "type": "is a concept used to understand", "evidence": "To attack the P = NP question, the concept of NP-completeness is very useful."}, {"source": "integer factorization problem", "target": "RSA algorithm", "type": "basis for", "evidence": "No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm."}, {"source": "polynomial algorithm", "target": "big o notation", "type": "practical limitations", "evidence": "A theoretical polynomial algorithm may have extremely large constant factors or exponents, rendering it impractical."}], "expandedNodes": []};

            document.addEventListener('DOMContentLoaded', function() {
                // Set up variables
                let expandedNodes = new Set(networkData.expandedNodes || []);
                let pinnedNodes = new Set(); // Track pinned/fixed nodes
                
                let currentSimulation = null;
                let currentNodes = [];
                
                let focusedNodeId = null;

                const width = 800;
                const height = 600;
                const centerX = width / 2;
                const centerY = height / 2;

                // Color scheme - shades of green from lightest to darkest
                const colorScheme = [
                    "#D1EDE8", "#ABD9D1", "#97C0DB", "#6596B5", "#9C82DE", "#9575CD"
                ];

                // Set up SVG and tooltips
                const svg = d3.select("#concept-map");
                const tooltip = d3.select("#tooltip");
                const evidenceTooltip = d3.select("#evidence-tooltip");

                // Create a group for zooming
                const g = svg.append("g");

                // Set up zoom behavior
                const zoom = d3.zoom()
                    .scaleExtent([0.5, 5])
                    .on("zoom", function(event) {
                        g.attr("transform", event.transform);
                    });

                svg.call(zoom);

                // Ensure nodes have string IDs and links reference those IDs
                networkData.nodes.forEach(node => {
                    node.id = String(node.id);
                });

                networkData.links.forEach(link => {
                    link.source = String(link.source);
                    link.target = String(link.target);
                });

                // Validate links - only keep links where both source and target nodes exist
                const validNodeIds = new Set(networkData.nodes.map(node => node.id));
                networkData.links = networkData.links.filter(link => 
                    validNodeIds.has(link.source) && validNodeIds.has(link.target)
                );

                // Function to calculate importance score
                function calculateImportanceScore(node) {
                    const frequencyWeight = 0.6;
                    const maxFrequency = Math.max(...networkData.nodes.map(n => n.frequency || 0), 1);
                    const maxDegree = Math.max(...networkData.nodes.map(n => n.degree || 0), 1);
                    const normalizedFrequency = (node.frequency || 0) / maxFrequency;
                    const normalizedDegree = (node.degree || 0) / maxDegree;
                    return (normalizedFrequency * frequencyWeight) + (normalizedDegree * (1 - frequencyWeight));
                }

                // Find the most important concept to place at the center
                function findCentralNode(nodes) {
                    return nodes.reduce((max, node) => {
                        const score = calculateImportanceScore(node);
                        return (score > calculateImportanceScore(max)) ? node : max;
                    }, nodes[0]);
                }

                // Function to get node size based on importance
                function getNodeSize(node) {
                    const score = calculateImportanceScore(node);
                    return 10 + (score * 30);  // Min 10, max 40
                }

                // Function to get node color based on layer
                function getNodeColor(node) {
                    let baseIntensity;
                    switch(node.layer) {
                        case 'priority':
                            baseIntensity = 5; // Darkest shade
                            break;
                        case 'secondary':
                            baseIntensity = 2; // Medium shade
                            break;
                        case 'tertiary':
                        default:
                            baseIntensity = 0; // Lightest shade
                    }

                    const score = calculateImportanceScore(node);
                    const intensityVariation = Math.min(Math.floor(score * 2), 1);
                    const colorIndex = Math.min(Math.max(baseIntensity - intensityVariation, 0), colorScheme.length - 1);

                    return colorScheme[colorIndex];
                }

                // Function to get visible data based on expanded nodes
                function getVisibleData() {
                    console.log("Getting visible data with expanded nodes:", Array.from(expandedNodes));
                    
                    // Priority nodes are always visible
                    const visibleNodeIds = new Set(
                        networkData.nodes
                            .filter(node => node.layer === "priority" || expandedNodes.has(node.id))
                            .map(node => node.id)
                    );
                    
                    console.log(`Initial visible nodes (priority + expanded): ${visibleNodeIds.size}`);
                    
                    // Keep track of how many nodes we add in this expansion pass
                    let nodesAdded = 0;
                    
                    // Find all nodes connected to expanded nodes
                    expandedNodes.forEach(expandedId => {
                        networkData.links.forEach(link => {
                            let sourceId, targetId;
                            
                            // Handle both string and object formats
                            if (typeof link.source === 'object') {
                                sourceId = link.source.id;
                            } else {
                                sourceId = String(link.source);
                            }
                            
                            if (typeof link.target === 'object') {
                                targetId = link.target.id;
                            } else {
                                targetId = String(link.target);
                            }
                            
                            if (sourceId === expandedId && !visibleNodeIds.has(targetId)) {
                                visibleNodeIds.add(targetId);
                                nodesAdded++;
                            }
                            
                            if (targetId === expandedId && !visibleNodeIds.has(sourceId)) {
                                visibleNodeIds.add(sourceId);
                                nodesAdded++;
                            }
                        });
                    });
                    
                    console.log(`Added ${nodesAdded} connected nodes to visible set`);
                    console.log(`Total visible nodes: ${visibleNodeIds.size}`);
                    
                    // Get visible nodes
                    const visibleNodes = networkData.nodes.filter(node => 
                        visibleNodeIds.has(node.id)
                    );
                    
                    // Get visible links
                    const visibleLinks = networkData.links.filter(link => {
                        let sourceId, targetId;
                        
                        // Handle both string and object formats
                        if (typeof link.source === 'object') {
                            sourceId = link.source.id;
                        } else {
                            sourceId = String(link.source);
                        }
                        
                        if (typeof link.target === 'object') {
                            targetId = link.target.id;
                        } else {
                            targetId = String(link.target);
                        }
                        
                        return visibleNodeIds.has(sourceId) && visibleNodeIds.has(targetId);
                    });
                    
                    console.log(`Visible nodes: ${visibleNodes.length}, Visible links: ${visibleLinks.length}`);
                    
                    return { nodes: visibleNodes, links: visibleLinks };
                }

                // Function to find nodes with hidden connections
                function getAllNodesWithHiddenConnections() {
                    // Get all nodes
                    const allNodeIds = new Set(networkData.nodes.map(n => n.id));
                    
                    // Get currently visible nodes
                    const visibleData = getVisibleData();
                    const visibleNodeIds = new Set(visibleData.nodes.map(n => n.id));
                    
                    // Build a map of all connections
                    const allConnections = new Map();
                    
                    // Initialize map with all nodes
                    allNodeIds.forEach(nodeId => {
                        allConnections.set(nodeId, []);
                    });
                    
                    // Add all connections
                    networkData.links.forEach(link => {
                        const sourceId = typeof link.source === 'object' ? link.source.id : String(link.source);
                        const targetId = typeof link.target === 'object' ? link.target.id : String(link.target);
                        
                        if (allConnections.has(sourceId)) {
                            allConnections.get(sourceId).push(targetId);
                        }
                        
                        if (allConnections.has(targetId)) {
                            allConnections.get(targetId).push(sourceId);
                        }
                    });
                    
                    // Find all nodes that have hidden connections
                    const nodesWithHidden = new Set();
                    
                    // Check each visible node
                    visibleNodeIds.forEach(nodeId => {
                        const connections = allConnections.get(nodeId) || [];

                        // If any connection is to a non-visible node, this node has hidden connections
                        if (connections.some(connId => !visibleNodeIds.has(connId))) {
                            nodesWithHidden.add(nodeId);
                        }
                    });

                    // Print debug info
                    console.log(`Found ${nodesWithHidden.size} nodes with hidden connections`);
                    console.log("Nodes with hidden connections:", Array.from(nodesWithHidden));

                    return nodesWithHidden;
                }

                // Function to assign initial positions in concentric circles
                function assignInitialPositions(nodes, centralNodeId) {
                    // Group nodes by layer
                    const layerGroups = {"priority": [], "secondary": [], "tertiary": []};
                    
                    // First pass - identify layers and pinned nodes
                    nodes.forEach(node => {
                        if (pinnedNodes.has(node.id)) {
                            console.log("Preserving position for pinned node:", node.id);
                            // Make sure fx and fy are set from existing position
                            node.fx = node.x;
                            node.fy = node.y;
                            return; // Skip further positioning for pinned nodes
                        }
                        
                        if (node.id === centralNodeId) {
                            // Central node stays at center
                            node.x = centerX;
                            node.y = centerY;
                            node.fx = centerX; // Fix position
                            node.fy = centerY; // Fix position
                            node.isCenter = true;
                        } else {
                            // Group other nodes by layer
                            const layer = node.layer || "tertiary";
                            layerGroups[layer].push(node);
                            // Clear any fixed positions for non-pinned nodes
                            if (!pinnedNodes.has(node.id)) {
                                node.fx = null;
                                node.fy = null;
                            }
                            node.isCenter = false;
                        }
                    });
                    
                    // Position nodes in evenly distributed concentric circles by layer
                    positionNodesInCircle(layerGroups["priority"], 120);
                    positionNodesInCircle(layerGroups["secondary"], 240);
                    positionNodesInCircle(layerGroups["tertiary"], 360);
                }

                // Helper function to position nodes in a circle
                function positionNodesInCircle(nodes, radius) {
                    const count = nodes.length;
                    if (count === 0) return;
                    
                    // Evenly distribute nodes around the circle
                    const angleStep = (2 * Math.PI) / count;
                    
                    // Use a randomized offset to avoid bias to any particular direction
                    const startAngle = Math.random() * 2 * Math.PI;
                    
                    nodes.forEach((node, i) => {
                        // Calculate angle with random offset to avoid clustering
                        const angle = startAngle + i * angleStep;
                        
                        // Position node on the circle
                        node.x = centerX + radius * Math.cos(angle);
                        node.y = centerY + radius * Math.sin(angle);
                    });
                }

                // Function to update the visualization
                function updateVisualization() {
                    // Get current data
                    const { nodes, links } = getVisibleData();
                    const nodesWithHidden = getAllNodesWithHiddenConnections();
                    
                    currentNodes = nodes;
                    currentLinks = links;

                    // Find central node
                    const centralNode = findCentralNode(nodes);

                    // Clear previous elements
                    g.selectAll("*").remove();

                    // Create a node ID lookup for the simulation
                    const nodeById = new Map(nodes.map(node => [node.id, node]));
                    
                    nodes.forEach(node => {
                        if (pinnedNodes.has(node.id)) {
                            // If this node is pinned, ensure it has fixed coordinates
                            const pinnedNode = nodeById.get(node.id);
                            if (pinnedNode) {
                                node.fx = pinnedNode.x || node.x;
                                node.fy = pinnedNode.y || node.y;
                            }
                        }
                    });

                    // Assign initial positions
                    assignInitialPositions(nodes, centralNode.id);

                    // Set up the simulation with proper node references and forces
                     const simulation = d3.forceSimulation(nodes)
                        .force("link", d3.forceLink()
                            .id(d => d.id)
                            .links(links.map(link => ({
                                source: nodeById.get(String(link.source)) || String(link.source),
                                target: nodeById.get(String(link.target)) || String(link.target),
                                type: link.type,
                                evidence: link.evidence
                            })))
                            .distance(d => {
                                // Adjust distance based on layer and node size
                                const source = typeof d.source === 'object' ? d.source : nodeById.get(String(d.source));
                                const target = typeof d.target === 'object' ? d.target : nodeById.get(String(d.target));
                        
                                if (!source || !target) return 120;
                        
                                // Get sizes of source and target nodes
                                const sourceSize = getNodeSize(source);
                                const targetSize = getNodeSize(target);
                                
                                // Base distance on node sizes + a minimum distance
                                const baseDistance = sourceSize + targetSize + 30;
                                
                                // Layer-based adjustments
                                if (source.layer === "priority" && target.layer === "priority") {
                                    // Priority-to-priority connections are slightly closer
                                    return baseDistance * 1.2;
                                } else if (source.layer === "priority" || target.layer === "priority") {
                                    // Priority-to-other connections at medium distance
                                    return baseDistance * 1.5;
                                }
                                
                                // Other connections have more space
                                return baseDistance * 2.0;
                            })
                            .strength(0.3))
                        .force("charge", d3.forceManyBody().strength(d => {
                            // Stronger repulsion for larger nodes
                            return d.isCenter ? -500 : -300;
                        }))
                        .force("center", d3.forceCenter(centerX, centerY).strength(0.2)) // Stronger centering force
                        .force("collide", d3.forceCollide().radius(d => getNodeSize(d) + 10))
                        .force("x", d3.forceX(centerX).strength(d => {
                            // Stronger x-centering for all nodes
                            if (d.isCenter) return 1.0;
                            if (d.layer === "priority") return 0.15; // Increased from 0.1
                            if (d.layer === "secondary") return 0.1; // Increased from 0.05
                            return 0.05; // Increased from 0.01
                        }))
                        .force("y", d3.forceY(centerY).strength(d => {
                            // Stronger y-centering for all nodes
                            if (d.isCenter) return 1.0;
                            if (d.layer === "priority") return 0.15;
                            if (d.layer === "secondary") return 0.1;
                            return 0.05;
                        }))
                        .force("radial", d3.forceRadial(d => {
                            // Target radius based on layer
                            if (d.layer === "priority") return 120;
                            if (d.layer === "secondary") return 240;
                            return 360; // tertiary
                        }, centerX, centerY).strength(0.15))
                        .force("link-repulsion", d3.forceManyBody()
                            .strength(-10)
                            .distanceMax(150)
                            .distanceMin(25))
                        .alphaDecay(0.02);
                    
                    currentSimulation = simulation;

                    // Create links with hover effects
                    const link = g.selectAll(".link")
                        .data(links)
                        .join("path")
                        .attr("class", "link")
                        .attr("stroke", function(d) {
                            // Get the target node
                            const target = typeof d.target === 'object' ? d.target : nodeById.get(String(d.target));
                            
                            if (!target) return "#BDBDBD"; // Default gray
                            
                            // Color based on target's layer
                            switch(target.layer) {
                                case "priority":
                                    return "#B39DDB"; // Purple for priority
                                case "secondary":
                                    return "#90CAF9"; // Blue for secondary
                                case "tertiary":
                                    return "#B2DFDB"; // Light blue/green for tertiary
                                default:
                                    return "#BDBDBD"; // Default gray
                            }
                        })
                        .attr("stroke-opacity", 0.6)
                        .attr("stroke-width", function(d) {
                            const sourceNode = nodes.find(n => n.id === String(d.source));
                            const targetNode = nodes.find(n => n.id === String(d.target));
                            return (sourceNode?.layer === "priority" && targetNode?.layer === "priority") ? 3 : 1.5;
                        })
                        .attr("fill", "none")
                        .on("mouseover", function(event, d) {
                            // Highlight the line on hover
                            const currentColor = d3.select(this).attr("stroke");
                            const sourceNode = typeof d.source === 'object' ? d.source : nodeById.get(String(d.source));
                            const targetNode = typeof d.target === 'object' ? d.target : nodeById.get(String(d.target));
                        
                            d3.select(this)
                                .attr("stroke-opacity", 1)
                                .attr("stroke-width", function() {
                                    return parseFloat(d3.select(this).attr("stroke-width")) + 1;
                                })
                                .attr("stroke", function() {
                                    // Darken the current color for hover effect
                                    const target = typeof d.target === 'object' ? d.target : nodeById.get(String(d.target));
                                    
                                    if (!target) return "#999";
                                    
                                    switch(target.layer) {
                                        case "priority":
                                            return "#9575CD"; // Slightly darker purple
                                        case "secondary":
                                            return "#64B5F6"; // Slightly darker blue
                                        case "tertiary":
                                            return "#80CBC4"; // Slightly darker teal
                                        default:
                                            return "#999"; // Darker gray
                                    }
                                });
                            
                            if (sourceNode && targetNode) {
                                evidenceTooltip
                                    .style("display", "block")
                                    .style("left", event.pageX + 10 + "px")
                                    .style("top", event.pageY - 10 + "px")
                                    .html(`
                                        <strong>${sourceNode.name || sourceNode.id}</strong>
                                        <span style="margin: 0 5px;">→</span>
                                        <strong>${d.type || "relates to"}</strong>
                                        <span style="margin: 0 5px;">→</span>
                                        <strong>${targetNode.name || targetNode.id}</strong>
                                        <hr style="margin: 8px 0;">
                                        <div>${d.evidence || "No evidence available"}</div>
                                    `);
                            };
                        })
                        .on("mouseout", function() {
                            // Restore original line style                            
                            d3.select(this)
                                    .attr("stroke-opacity", 0.6)
                                    .attr("stroke-width", function(d) {
                                        const target = typeof d.target === 'object' ? d.target : nodeById.get(String(d.target));
                                        
                                        if (target && target.layer === "priority") return 2;
                                        if (target && target.layer === "secondary") return 1.8;
                                        return 1.5;
                                    })
                                    .attr("stroke", function(d) {
                                        // Restore original color
                                        const target = typeof d.target === 'object' ? d.target : nodeById.get(String(d.target));
                                        
                                        if (!target) return "#BDBDBD";
                                        
                                        switch(target.layer) {
                                            case "priority":
                                                return "#B39DDB"; // Light purple
                                            case "secondary":
                                                return "#90CAF9"; // Light blue
                                            case "tertiary":
                                                return "#B2DFDB"; // Light teal
                                            default:
                                                return "#BDBDBD";
                                        }
                                    });

                            // Hide the evidence tooltip
                            evidenceTooltip.style("display", "none");
                        });

                    // Create node groups
                    const node = g.selectAll(".node")
                        .data(nodes)
                        .join("g")
                        .attr("class", function(d) {
                            return "node node--" + (d.layer || "tertiary") + 
                                   (expandedNodes.has(d.id) ? " node--expanded" : "") +
                                   (d.isCenter ? " center-node" : "");
                        });
                    
                    node
                        // Left-click for expanding/collapsing hidden connections
                        // Update the node click handler
                        .on("click", function(event, d) {
                            event.stopPropagation();
                            
                            // Store current positions of all nodes
                            const nodePositions = new Map();
                            currentNodes.forEach(node => {
                                nodePositions.set(node.id, {x: node.x, y: node.y});
                            });
                        
                            if (nodesWithHidden.has(d.id)) {
                                // First apply focus immediately regardless of expansion state
                                applyFocus(d.id);
                                focusedNodeId = d.id;
                                
                                // Toggle expansion state
                                if (expandedNodes.has(d.id)) {
                                    // COLLAPSING - The node is already expanded, so collapse it
                                    expandedNodes.delete(d.id);
                                    pinnedNodes.delete(d.id);
                                    d.fx = null;
                                    d.fy = null;
                                    d3.select(this).classed("node--pinned", false);
                                    
                                    // Update visualization (removes hidden connections)
                                    updateVisualization();
                                    
                                    // After visualization update, reapply focus to remaining nodes
                                    setTimeout(() => {
                                        if (focusedNodeId) {
                                            applyFocus(focusedNodeId);
                                        }
                                    }, 50);
                                } else {
                                    // EXPANDING - Node isn't expanded yet, so expand it
                                    expandedNodes.add(d.id);
                                    pinnedNodes.add(d.id);
                                    d.fx = d.x;
                                    d.fy = d.y;
                                    d3.select(this).classed("node--pinned", true);
                                    
                                    // Update visualization to show hidden connections
                                    updateVisualization();
                                    
                                    // After updating, fix positions and reapply focus
                                    setTimeout(() => {
                                        g.selectAll(".node").each(function(node) {
                                            const oldPos = nodePositions.get(node.id);
                                            if (oldPos) {
                                                // Fix this node at its previous position temporarily
                                                node.fx = oldPos.x;
                                                node.fy = oldPos.y;
                                            }
                                            
                                            // Keep expanded nodes pinned
                                            if (pinnedNodes.has(node.id)) {
                                                node.fx = node.x;
                                                node.fy = node.y;
                                            }
                                        });
                                        
                                        // Run simulation to position new nodes
                                        currentSimulation.alpha(0.3).restart();
                                        
                                        // Release non-pinned nodes after a delay
                                        setTimeout(() => {
                                            g.selectAll(".node").each(function(node) {
                                                if (!pinnedNodes.has(node.id) && !node.isCenter) {
                                                    node.fx = null;
                                                    node.fy = null;
                                                }
                                            });
                                            
                                            currentSimulation.alpha(0.05).restart();
                                            
                                            // Reapply focus to include new connections
                                            if (focusedNodeId) {
                                                applyFocus(focusedNodeId);
                                            }
                                        }, 1500);
                                    }, 50);
                                }
                            
                                sendMessageToStreamlit({
                                    expandedNodes: Array.from(expandedNodes)
                                });
                            }
                            // Case 2: Node doesn't have hidden connections - just toggle focus
                            else {
                                // If already focused, unfocus
                                if (focusedNodeId === d.id) {
                                    focusedNodeId = null;
                                    resetFocus();
                                } 
                                // Otherwise set focus to this node
                                else {
                                    focusedNodeId = d.id;
                                    // Pin the node temporarily while focused
                                    if (d && !d.isCenter) {
                                        d.fx = d.x;
                                        d.fy = d.y;
                                    }
                                    // Apply visual focus immediately
                                    applyFocus(d.id);
                                }
                            }
                        
                            // Try to sync with Streamlit if needed
                            sendMessageToStreamlit({
                                expandedNodes: Array.from(expandedNodes)
                            });
                        })
                        // Right-click (contextmenu) for concept explanation
                        .on("contextmenu", function(event, d) {
                            // Prevent the default context menu
                            event.preventDefault();
                            
                            // Get the evidence for this concept
                            const nodeData = networkData.nodes.find(n => n.id === d.id);
                            const evidence = nodeData.evidence || "No explanation available for this concept.";
                                                        
                            // Create or update the explanation panel
                            if (!d3.select("#explanation-panel").size()) {
                                d3.select("body").append("div")
                                    .attr("id", "explanation-panel")
                                    .style("position", "absolute")
                                    .style("padding", "15px")
                                    .style("background", "white")
                                    .style("border", "1px solid #ccc")
                                    .style("border-radius", "8px")
                                    .style("box-shadow", "0 2px 10px rgba(0,0,0,0.2)")
                                    .style("max-width", "300px")
                                    .style("z-index", "1000")
                                    .style("font-size", "14px")
                                    .style("line-height", "1.4");
                                    
                                // Add close button
                                d3.select("#explanation-panel")
                                    .append("button")
                                    .attr("class", "close-explanation")
                                    .style("position", "absolute")
                                    .style("top", "5px")
                                    .style("right", "5px")
                                    .style("background", "none")
                                    .style("border", "none")
                                    .style("font-size", "16px")
                                    .style("cursor", "pointer")
                                    .style("color", "#666")
                                    .html("&times;")
                                    .on("click", function() {
                                        d3.select("#explanation-panel").style("display", "none");
                                    });
                            }
                            
                            // Update and position the explanation panel
                            d3.select("#explanation-panel")
                                .style("display", "block")
                                .style("left", (event.pageX + 10) + "px")
                                .style("top", (event.pageY - 10) + "px")
                                .html(`
                                    <button class="close-explanation" style="position:absolute;top:5px;right:5px;background:none;border:none;font-size:16px;cursor:pointer;color:#666;">&times;</button>
                                    <div style="margin-top: 5px;">
                                        <h3 style="margin-top:0;margin-bottom:10px;color:#2196F3;">${d.name || d.id}</h3>
                                        <p>${evidence}</p>
                                        <span style="display:block;margin-top:8px;font-style:italic;color:#666;font-size:12px;">Layer: ${d.layer || "unknown"}</span>
                                    </div>
                                `);
                                
                            // Handle close button click
                            d3.select(".close-explanation").on("click", function() {
                                d3.select("#explanation-panel").style("display", "none");
                            });
                                
                            // Visual feedback for right-click
                            d3.select(this).select("circle")
                                .transition()
                                .duration(200)
                                .attr("r", function(d) { return getNodeSize(d) * 1.2; })
                                .transition()
                                .duration(200)
                                .attr("r", function(d) { return getNodeSize(d); });
                        })
                        .on("mouseover", function(event, d) {
                            // Show basic node info on hover with updated instructions
                            tooltip
                                .style("display", "block")
                                .style("left", (event.pageX + 10) + "px")
                                .style("top", (event.pageY - 10) + "px")
                                .html("<strong>" + (d.name || d.id) + "</strong><br>" +
                                      "<em>Level: " + (d.layer || "unknown") + "</em><br>" +
                                      "<em>Frequency: " + (d.frequency || 0) + "</em><br>" +
                                      "<em>Connections: " + (d.degree || 0) + "</em>" +
                                      (nodesWithHidden.has(d.id)
                                          ? "<br><span style='color:#FF8A65'><em>Click to expand hidden connections" + 
                                            (pinnedNodes.has(d.id) ? " (pinned)" : "") + "</em></span>"
                                          : "<br><span style='color:#2196F3'><em>Click to focus on this node's connections</em></span>") +
                                      "<br><span style='color:#2196F3'><em>Right-click for explanation</em></span>");
                        })
                        .on("mouseout", function() {
                            tooltip.style("display", "none");
                        })
                        .call(d3.drag()
                            .on("start", dragstarted)
                            .on("drag", dragged)
                            .on("end", dragended));

                    // Add circles to nodes with filled colors
                    const nodeCircles = node.append("circle")
                        .attr("r", function(d) { return getNodeSize(d); })
                        .attr("fill", function(d) { return getNodeColor(d); })  // Use color for fill
                        .attr("stroke", "#7E57C2");  // White stroke by default

                    // Add pulse animation to nodes with hidden connections
                    node.filter(function(d) { return nodesWithHidden.has(d.id); })
                        .append("circle")
                        .attr("r", function(d) { return getNodeSize(d) + 1; })
                        .attr("fill", "none")
                        .style("stroke", "#FF8A65")
                        .style("stroke-width", "2px")
                        .attr("class", "pulse-ring")
                        // Crucially, store the node ID as a data attribute
                        .attr("data-node-id", function(d) { return d.id; })
                        .call(function(selection) {
                            // Define the pulse animation function
                            function pulse() {
                                selection
                                    .transition()
                                    .duration(1000)
                                    // Only transform the radius, not the opacity
                                    .attr("r", function(d) { return getNodeSize(d) + 6; })
                                    .transition()
                                    .duration(1000)
                                    .attr("r", function(d) { return getNodeSize(d) + 5; })
                                    .on("end", pulse);
                            }
                            // Start the pulse animation
                            pulse();
                        });

                    // Add text labels to nodes
                    node.append("text")
                        .attr("class", "node-label")
                        .attr("text-anchor", "middle")
                        .attr("font-size", function(d) { 
                            if (d.isCenter) return "14px";
                            return d.layer === "priority" ? "12px" : "10px"; 
                        })
                        .attr("font-weight", function(d) { 
                            if (d.isCenter) return "bold";
                            return d.layer === "priority" ? "bold" : "normal";
                        })
                        .attr("fill", "#000000")
                        .attr("opacity", function(d) {
                            // Show all labels for priority nodes, but fewer labels for other layers
                            return d.layer === "priority" ? 1 : 0.7;
                        })
                        .text(function(d) {
                            return d.name || d.id;
                        })
                        .each(function(d) {
                            // Get label width to improve positioning
                            const bbox = this.getBBox();
                            d.labelWidth = bbox.width;
                            d.labelHeight = bbox.height;
                        });
                    
                    node.selectAll("text")
                        .each(function(d) {
                            // Get accurate bounding box for each label
                            const bbox = this.getBBox();
                            d.labelWidth = bbox.width;
                            d.labelHeight = bbox.height;
                            // Store expanded bounding box for collision detection
                            d.labelBBox = {
                                x: -bbox.width/2 - 5,  // Add padding
                                y: -bbox.height/2 - 2, // Add padding
                                width: bbox.width + 10,
                                height: bbox.height + 4
                            };
                        });
                                        
                    // Add a stronger collision detection force specifically for labels
                    simulation.force("label-collision", d3.forceCollide()
                        .radius(function(d) {
                            // Calculate collision radius based on node size and label dimensions
                            const nodeRadius = getNodeSize(d);
                            // Use the max of label width and height divided by 2 for the collision radius
                            const labelWidth = d.labelWidth || 0;
                            const labelHeight = d.labelHeight || 0;
                            const labelRadius = Math.max(labelWidth, labelHeight) / 1.8;
                            
                            // Return the larger of node radius or label radius, plus padding
                            return Math.max(nodeRadius, labelRadius) + 15;
                        })
                        .strength(0.8) // Stronger collision force specifically for labels
                        .iterations(3) 
                    );

                    // Update simulation
                    simulation.alpha(1).restart(); // Full restart for better layout
                    
                    node.isPinned = true;

                    simulation.on("tick", function() {
                        nodes.forEach(d => {
                            if (pinnedNodes.has(d.id) || d.isPinned) {
                                if (d.fx !== null && d.fy !== null) {
                                    d.x = d.fx;
                                    d.y = d.fy;
                                }
                            }
                            
                            if (pinnedNodes.has(d.id) && (d.fx === null || d.fy === null)) {
                                console.warn("Pinned node lost its fixed position:", d.id);
                            }
                            
                            if (d.isCenter) {
                                d.x = centerX;
                                d.y = centerY;
                            }

                            // Apply gentle force to keep nodes in their layer rings
                            if (!d.isCenter) {
                                // Calculate distance from center
                                const dx = d.x - centerX;
                                const dy = d.y - centerY;
                                const distance = Math.sqrt(dx * dx + dy * dy);

                                // Target radius based on layer
                                let targetRadius;
                                if (d.layer === "priority") targetRadius = 120;
                                else if (d.layer === "secondary") targetRadius = 240;
                                else targetRadius = 360; // tertiary

                                // Strength of the force (adjust as needed)
                                const strength = 0.05;

                                if (distance > 0) {
                                    // Push/pull toward the target radius
                                    const factor = 1 - (targetRadius / distance);
                                    d.x -= dx * factor * strength;
                                    d.y -= dy * factor * strength;
                                }
                            }
                        });

                        link.attr("d", function(d) {
                            const source = typeof d.source === 'object' ? d.source : nodeById.get(String(d.source));
                            const target = typeof d.target === 'object' ? d.target : nodeById.get(String(d.target));
                            
                            if (!source || !target) return "";
                            
                            // Find other links between the same nodes
                            const relatedLinks = links.filter(l => 
                                (l.source.id === source.id && l.target.id === target.id) ||
                                (l.source.id === target.id && l.target.id === source.id)
                            );
                            
                            // Calculate midpoint
                            const midX = (source.x + target.x) / 2;
                            const midY = (source.y + target.y) / 2;
                            
                            // Calculate normal vector for curve control point
                            const dx = target.x - source.x;
                            const dy = target.y - source.y;
                            const normalX = -dy;
                            const normalY = dx;
                            
                            // Normalize and scale for curvature
                            const len = Math.sqrt(normalX * normalX + normalY * normalY);
                            let curvature = 0;
                            
                            if (len > 0) {
                                // If multiple relationships, adjust curvature for each
                                const relationIndex = relatedLinks.indexOf(d);
                                const multiplier = relationIndex === 0 ? 1 : 1 + (relationIndex * 0.5);
                                curvature = 20 * multiplier;
                            }
                            
                            const controlX = midX + (normalX / len) * curvature;
                            const controlY = midY + (normalY / len) * curvature;
                            
                            // Quadratic curve path
                            return `M${source.x},${source.y} Q${controlX},${controlY} ${target.x},${target.y}`;
                        });
                        
                        node.attr("transform", function(d) {
                            return "translate(" + d.x + "," + d.y + ")";
                        });
                        
                        node.select("text")
                            .attr("dy", function(d) {
                                // Check surrounding density
                                let nearbyNodes = 0;
                                const threshold = getNodeSize(d) * 4; // Expanded detection radius
                                let crowdedTop = 0, crowdedBottom = 0, crowdedLeft = 0, crowdedRight = 0;
                                
                                nodes.forEach(other => {
                                    if (d.id !== other.id) {
                                        const dx = d.x - other.x;
                                        const dy = d.y - other.y;
                                        const distance = Math.sqrt(dx*dx + dy*dy);
                                        
                                        if (distance < threshold) {
                                            nearbyNodes++;
                                            // Check which direction is most crowded
                                            if (Math.abs(dx) > Math.abs(dy)) {
                                                // Horizontal proximity
                                                if (dx > 0) crowdedLeft++; else crowdedRight++;
                                            } else {
                                                // Vertical proximity
                                                if (dy > 0) crowdedTop++; else crowdedBottom++;
                                            }
                                        }
                                    }
                                });
                                
                                const dirs = [
                                    {dir: "top", count: crowdedTop, offset: -getNodeSize(d) - 10},
                                    {dir: "right", count: crowdedRight, offset: "0.35em"},
                                    {dir: "bottom", count: crowdedBottom, offset: getNodeSize(d) + 14},
                                    {dir: "left", count: crowdedLeft, offset: "0.35em"}
                                ];
                                
                                dirs.sort((a, b) => a.count - b.count);

                                // Adaptively position label based on node density and position
                                const angle = Math.atan2(d.y - centerY, d.x - centerX);

                                // If crowded area, place labels more carefully
                                if (d.layer === "priority" || nearbyNodes <= 2) {
                                    if (angle > -Math.PI/4 && angle < Math.PI/4) {
                                        return "0.35em"; // Right side
                                    } else if (angle >= Math.PI/4 && angle < 3*Math.PI/4) {
                                        return getNodeSize(d) + 14; // Below
                                    } else if (angle >= 3*Math.PI/4 || angle <= -3*Math.PI/4) {
                                        return "0.35em"; // Left side
                                    } else {
                                        return -getNodeSize(d) - 10; // Above
                                    }
                                }
                                
                                // Otherwise use the least crowded direction
                                return dirs[0].offset;
                            })
                            .attr("dx", function(d) {
                                // Similar to dy logic, but for horizontal positioning
                                const angle = Math.atan2(d.y - centerY, d.x - centerX);
                                
                                // Use existing dx logic but with more spacing
                                if (angle > -Math.PI/4 && angle < Math.PI/4) {
                                    return getNodeSize(d) + 8; // To the right
                                } else if (angle >= Math.PI/4 && angle < 3*Math.PI/4) {
                                    return 0; // Centered horizontally
                                } else if (angle >= 3*Math.PI/4 || angle <= -3*Math.PI/4) {
                                    return -getNodeSize(d) - 8; // To the left
                                } else {
                                    return 0; // Centered horizontally
                                }
                            })
                            .attr("text-anchor", function(d) {
                                const angle = Math.atan2(d.y - centerY, d.x - centerX);

                                // Set text anchor based on angle
                                if (angle > -Math.PI/4 && angle < Math.PI/4) {
                                    return "start"; // Right side
                                } else if (angle >= Math.PI/4 && angle < 3*Math.PI/4) {
                                    return "middle"; // Bottom
                                } else if (angle >= 3*Math.PI/4 || angle <= -3*Math.PI/4) {
                                    return "end"; // Left side
                                } else {
                                    return "middle"; // Top
                                }
                            });
                    }); 
                    
                    // Drag functions
                    function dragstarted(event, d) {
                        if (d.isCenter) return; // Don't allow dragging center node
                        if (!event.active) simulation.alphaTarget(0.3).restart();
                        // Store original position
                        d._originalX = d.x;
                        d._originalY = d.y;
                        
                        // Always fix position during drag
                        d.fx = d.x;
                        d.fy = d.y;
                    }
                            
                    function dragged(event, d) {
                        if (d.isCenter) return; // Don't allow dragging center node
                        d.fx = event.x;
                        d.fy = event.y;
                    }
                
                    function dragended(event, d) {
                        if (d.isCenter) return; // Don't allow dragging center node
                        if (!event.active) simulation.alphaTarget(0);
                        // If this node is pinned, keep it fixed at the new position
                        if (pinnedNodes.has(d.id)) {
                            d.fx = d.x;
                            d.fy = d.y;
                            console.log("Node remains pinned after drag:", d.id, "at", d.x, d.y);
                        } else {
                            // Otherwise, release it
                            d.fx = null;
                            d.fy = null;
                        }
                    }
                }
                
                function applyFocus(nodeId) {
                    console.log("Applying focus to node:", nodeId);
                    
                    // Get the focused node and its direct connections
                    const connectedNodeIds = new Set();
                    connectedNodeIds.add(nodeId); // Add the focused node itself
                    
                    // Use DOM selection to find connected nodes
                    g.selectAll(".link").each(function(link) {
                        let sourceId, targetId;
                        
                        // Handle all possible formats of source/target
                        if (typeof link.source === 'object' && link.source !== null) {
                            sourceId = link.source.id;
                        } else {
                            sourceId = String(link.source);
                        }
                        
                        if (typeof link.target === 'object' && link.target !== null) {
                            targetId = link.target.id;
                        } else {
                            targetId = String(link.target);
                        }
                        
                        // Add connected nodes to our set
                        if (sourceId === nodeId) {
                            connectedNodeIds.add(targetId);
                        }
                        if (targetId === nodeId) {
                            connectedNodeIds.add(sourceId);
                        }
                    });
                                        
                    // Apply fading and highlighting using DOM selections
                    g.selectAll(".node").classed("faded", function(d) {
                        return !connectedNodeIds.has(d.id);
                    });
                    
                    g.selectAll(".pulse-ring").each(function() {
                        const ringNodeId = d3.select(this).attr("data-node-id");
                        
                        // Set appropriate opacity based on connection
                        if (connectedNodeIds.has(ringNodeId)) {
                            d3.select(this).style("opacity", 0.5);  // Connected - visible
                            
                            // Clear any fading class if present
                            d3.select(this).classed("faded-ring", false);
                        } else {
                            d3.select(this).style("opacity", 0.01);  // Not connected - faded
                            
                            // Add class to mark as faded
                            d3.select(this).classed("faded-ring", true);
                        }
                    });
                    
                    g.selectAll(".link").classed("faded", function(d) {
                        let sourceId, targetId;
                        
                        if (typeof d.source === 'object' && d.source !== null) {
                            sourceId = d.source.id;
                        } else {
                            sourceId = String(d.source);
                        }
                        
                        if (typeof d.target === 'object' && d.target !== null) {
                            targetId = d.target.id;
                        } else {
                            targetId = String(d.target);
                        }
                        
                        return !(connectedNodeIds.has(sourceId) && connectedNodeIds.has(targetId));
                    });
                    
                    // Highlight the focused node
                    g.selectAll(".node").filter(d => d.id === nodeId).classed("focused", true);
                    
                }

                function resetFocus() {
                    console.log("Resetting focus");
                    g.selectAll(".node").classed("faded", false).classed("focused", false);
                    g.selectAll(".link").classed("faded", false);
                    g.selectAll(".pulse-ring").style("opacity", "0.5");
                }
                
                // Button click handlers
                d3.select("#reset-btn").on("click", function() {
                    expandedNodes.clear();
                    updateVisualization();
                    sendMessageToStreamlit([]);
                });
                
                d3.select("#expand-all-btn").on("click", function() {
                    console.log("Expand All button clicked");
                    
                    // First get all nodes with hidden connections
                    const nodesWithHidden = getAllNodesWithHiddenConnections();
                    
                    // Expand them all at once
                    let expansionsAdded = 0;
                    nodesWithHidden.forEach(nodeId => {
                        if (!expandedNodes.has(nodeId)) {
                            expandedNodes.add(nodeId);
                            expansionsAdded++;
                        }
                    });
                    
                    console.log(`Added ${expansionsAdded} nodes to expanded set`);
                    console.log("Expanded nodes:", Array.from(expandedNodes));
                    
                    // Update visualization with the new expanded set
                    updateVisualization();
                    
                    // Try to communicate with Streamlit if available
                    if (window.Streamlit) {
                        try {
                            window.Streamlit.setComponentValue({expandedNodes: Array.from(expandedNodes)});
                            console.log("Sent expanded nodes to Streamlit");
                        } catch (e) {
                            console.error("Error sending to Streamlit:", e);
                        }
                    } else {
                        console.warn("Streamlit object not available");
                    }
                });
                
                d3.select("#reset-btn").on("click", function() {
                    console.log("Reset button clicked");
                    
                    // Clear expanded nodes
                    const previousCount = expandedNodes.size;
                    expandedNodes.clear();
                    
                    console.log(`Cleared ${previousCount} expanded nodes`);
                    
                    // Update visualization
                    updateVisualization();
                    
                    // Try to communicate with Streamlit if available
                    if (window.Streamlit) {
                        try {
                            window.Streamlit.setComponentValue({expandedNodes: []});
                            console.log("Sent empty expanded nodes to Streamlit");
                        } catch (e) {
                            console.error("Error sending to Streamlit:", e);
                        }
                    } else {
                        console.warn("Streamlit object not available");
                    }
                });
                
                // Update the unpin button handler for gradual repositioning
                d3.select("#unpin-btn").on("click", function() {
                    console.log("Unpin All button clicked");
                    
                    if (!currentNodes || !currentSimulation) {
                        console.error("No active visualization");
                        return;
                    }
                    
                    // Store original positions before unpinning
                    const originalPositions = new Map();
                    currentNodes.forEach(node => {
                        originalPositions.set(node.id, {x: node.x, y: node.y});
                    });
                    
                    // Unpin all nodes
                    pinnedNodes.forEach(nodeId => {
                        const node = currentNodes.find(n => n.id === nodeId);
                        if (node && !node.isCenter) {
                            console.log("Unpinning node:", nodeId);
                            // Release fixed position
                            node.fx = null;
                            node.fy = null;
                        }
                    });
                    
                    // Clear pinned nodes set
                    pinnedNodes.clear();
                    
                    // Remove visual indicators
                    g.selectAll(".node--pinned").classed("node--pinned", false);
                    
                    // Reset focus as well
                    focusedNodeId = null;
                    resetFocus();
                    
                    // First stage: Gentle transition from current positions
                    currentSimulation.alpha(0.2).restart();
                    
                    // Second stage: Apply layer-based positioning after a short delay
                    setTimeout(() => {
                        // Calculate target positions based on layers
                        currentNodes.forEach(node => {
                            // Skip the center node
                            if (node.isCenter) return;
                            
                            // Calculate target radius based on layer
                            let targetRadius;
                            if (node.layer === "priority") targetRadius = 120;
                            else if (node.layer === "secondary") targetRadius = 240;
                            else targetRadius = 360; // tertiary
                            
                            // Get original position
                            const origPos = originalPositions.get(node.id);
                            if (!origPos) return;
                            
                            // Calculate angle from center
                            const dx = origPos.x - centerX;
                            const dy = origPos.y - centerY;
                            const currentAngle = Math.atan2(dy, dx);
                            
                            // Calculate new position based on layer radius
                            const newX = centerX + targetRadius * Math.cos(currentAngle);
                            const newY = centerY + targetRadius * Math.sin(currentAngle);
                            
                            // Apply a gentle pull toward the target position
                            const pullStrength = 0.1;
                            node.vx = (newX - node.x) * pullStrength;
                            node.vy = (newY - node.y) * pullStrength;
                        });
                        
                        // Run simulation with higher alpha for reorganization
                        currentSimulation.alpha(0.3).restart();
                        
                        // Reset any forces to allow natural positioning
                        setTimeout(() => {
                            currentNodes.forEach(node => {
                                // Remove velocity modifications
                                node.vx = null;
                                node.vy = null;
                            });
                            
                            // Final gentle adjustment
                            currentSimulation.alpha(0.1).restart();
                        }, 1000);
                    }, 500);
                });
                
                // Function to communicate with Streamlit
                function safelySendMessageToStreamlit(message) {
                    console.log("Attempting to send message to Streamlit:", message);
                    
                    try {
                        // Check if Streamlit is available
                        if (window.Streamlit) {
                            window.Streamlit.setComponentValue(message);
                            console.log("Message sent successfully to Streamlit");
                            return true;
                        } else {
                            console.warn("Streamlit object not available yet. Will retry in 500ms");
                            
                            // Retry after a short delay
                            setTimeout(() => {
                                if (window.Streamlit) {
                                    window.Streamlit.setComponentValue(message);
                                    console.log("Message sent successfully to Streamlit on retry");
                                } else {
                                    console.error("Streamlit object still not available after retry");
                                    
                                    // Fall back to direct update if Streamlit communication fails
                                    try {
                                        expandedNodes = new Set(message.expandedNodes || []);
                                        updateVisualization();
                                        console.log("Applied changes locally since Streamlit communication failed");
                                    } catch (localError) {
                                        console.error("Error applying local changes:", localError);
                                    }
                                }
                            }, 500);
                            return false;
                        }
                    } catch (error) {
                        console.error("Error sending message to Streamlit:", error);
                        return false;
                    }
                }
                
                function sendMessageToStreamlit(message) {
                    // Only proceed if we're in a Streamlit context
                    if (window.Streamlit && window.Streamlit.setComponentValue) {
                        try {
                            window.Streamlit.setComponentValue(message);
                            console.log("Message sent to Streamlit:", message);
                            return true;
                        } catch (e) {
                            console.error("Error sending to Streamlit:", e);
                            // No need to retry - just apply changes locally
                            console.log("Applying changes locally due to error");
                            return false;
                        }
                    } else {
                        console.log("Streamlit API not available, applying changes locally");
                        // No need to worry about it - all changes are already applied locally
                        return false;
                    }
                }
                
                svg.on("click", function(event) {
                    // Ignore if the click was on a node or a control
                    if (event.target.closest(".node") || event.target.closest(".controls")) 
                        return;
                    
                    // Reset focus
                    focusedNodeId = null;
                    resetFocus();
                    
                    // Unpin all nodes - access currentNodes instead of nodes
                    if (pinnedNodes.size > 0) {
                        pinnedNodes.forEach(nodeId => {
                            const node = currentNodes.find(n => n.id === nodeId);
                            if (node && !node.isCenter) {
                                node.fx = null;
                                node.fy = null;
                            }
                        });
                        
                        // Clear pinned nodes set
                        pinnedNodes.clear();
                        
                        // Remove visual indicators
                        g.selectAll(".node--pinned").classed("node--pinned", false);
                        
                        // Run simulation with low alpha to adjust
                        if (currentSimulation) {
                            currentSimulation.alpha(0.1).restart();
                        }
                    }
                });

                
                // Initial visualization
                updateVisualization();
                
                document.addEventListener('click', function(event) {
                    // Check if the click is outside the explanation panel and nodes
                    const explanationPanel = document.getElementById('explanation-panel');
                    const isClickOutsidePanel = explanationPanel && 
                                                !explanationPanel.contains(event.target) && 
                                                !event.target.closest('.node');
                    
                    if (isClickOutsidePanel) {
                        // Hide the explanation panel
                        d3.select("#explanation-panel").style("display", "none");
                    }
                });
            });
            </script>
        </body>
        </html>
        